{"cells":[{"cell_type":"markdown","source":["# Simple Word Window Classification\n","\n","### Build a basic network to solve an example NLP task.\n","\n","In this section, our goal will be to train a model that will find the words in a sentence corresponding to a `LOCATION`, which will be always of span `1`. This task is called `Word Window Classification`. Instead of letting our model to only take a look at one word in each forward pass, we would like it to be able to consider the context of the word in question. That is, for each word, we want our model to be aware of the surrounding words.\n","\n","\n","\n","\n","> referenced by CS224N\n","\n"],"metadata":{"id":"Vb4KWgG2DbR4"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn"],"metadata":{"id":"9xfi55Z1I4iw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Our raw data, which consists of sentences\n","corpus = [\n","          \"We always come to Paris\",\n","          \"The professor is from Australia\",\n","          \"I live in Stanford\",\n","          \"He comes from Taiwan\",\n","          \"The capital of Turkey is Ankara\"\n","         ]\n","\n","# Create our training set\n","train_sentences = [sent.lower().split() for sent in corpus]\n","\n","# Our train labels\n","locations = set([\"australia\", \"ankara\", \"paris\", \"stanford\", \"taiwan\", \"turkey\"])\n","train_labels = [[1 if word in locations else 0 for word in sent] for sent in train_sentences]\n","\n","# Find all the unique words in our corpus\n","vocabulary = set(w for s in train_sentences for w in s)\n","vocabulary.add(\"<unk>\")\n","vocabulary.add(\"<pad>\")\n","\n","# Creating a dictionary to find the index of a given word\n","ix_to_word = sorted(list(vocabulary))\n","word_to_ix = {word: ind for ind, word in enumerate(ix_to_word)}"],"metadata":{"id":"ouTI6HjUD0vA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Batching Sentences\n","\n","We have learned about batches in class. Waiting our whole training corpus to be processed before making an update is constly. On the other hand, updating the parameters after every training example causes the loss to be less stable between updates. To combat these issues, we instead update our parameters after training on a batch of data. This allows us to get a better estimate of the gradient of the global loss."],"metadata":{"id":"0t0MZI3BRCoC"}},{"cell_type":"code","source":["from torch.utils.data import DataLoader\n","from functools import partial\n","\n","def custom_collate_fn(batch, window_size, word_to_ix):\n","\n","  def pad_window(sentence, window_size, pad_token=\"<pad>\"):\n","    window = [pad_token] * window_size\n","    return window + sentence + window\n","\n","  def convert_tokens_to_indices(sentence, word_to_ix):\n","    return [word_to_ix.get(token, word_to_ix[\"<unk>\"]) for token in sentence]\n","\n","\n","  # Prepare the datapoints\n","  x, y = zip(*batch)\n","  x = [pad_window(s, window_size=window_size) for s in x]\n","  x = [convert_tokens_to_indices(s, word_to_ix) for s in x]\n","\n","  # Pad x so that all the examples in the batch have the same size\n","  pad_token_ix = word_to_ix[\"<pad>\"]\n","  x = [torch.LongTensor(x_i) for x_i in x]\n","  x_padded = nn.utils.rnn.pad_sequence(x, batch_first=True, padding_value=pad_token_ix)\n","\n","  # Pad y and record the length\n","  lengths = [len(label) for label in y]\n","  lenghts = torch.LongTensor(lengths)\n","  y = [torch.LongTensor(y_i) for y_i in y]\n","  y_padded = nn.utils.rnn.pad_sequence(y, batch_first=True, padding_value=0)\n","\n","  return x_padded, y_padded, lenghts"],"metadata":{"id":"tGGa1QkGQolQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Model\n","\n","Now that we have prepared our data, we are ready to build our model. We have learned how to write custom `nn.Module` classes. We will do the same here and put everything we have learned so far together."],"metadata":{"id":"yu1MrdwESJR_"}},{"cell_type":"code","source":["class WordWindowClassifier(nn.Module):\n","\n","  def __init__(self, hyperparameters, vocab_size, pad_ix=0):\n","    super(WordWindowClassifier, self).__init__()\n","\n","    \"\"\" Instance variables \"\"\"\n","    self.window_size = hyperparameters[\"window_size\"]\n","    self.embed_dim = hyperparameters[\"embed_dim\"]\n","    self.hidden_dim = hyperparameters[\"hidden_dim\"]\n","    self.freeze_embeddings = hyperparameters[\"freeze_embeddings\"]\n","\n","    self.embeds = nn.Embedding(vocab_size, self.embed_dim, padding_idx=pad_ix)\n","    if self.freeze_embeddings:\n","      self.embed_layer.weight.requires_grad = False\n","\n","\n","    full_window_size = 2 * window_size + 1\n","    self.hidden_layer = nn.Sequential(\n","      nn.Linear(full_window_size * self.embed_dim, self.hidden_dim),\n","      nn.Tanh()\n","    )\n","\n","\n","    self.output_layer = nn.Linear(self.hidden_dim, 1)\n","    self.probabilities = nn.Sigmoid()\n","\n","  def forward(self, inputs):\n","\n","    B, L = inputs.size()\n","\n","\n","    # Fist, get our word windows for each word in our input.\n","    token_windows = inputs.unfold(1, 2 * self.window_size + 1, 1)\n","    _, adjusted_length, _ = token_windows.size()\n","\n","    # Good idea to do internal tensor-size sanity checks, at the least in comments!\n","    assert token_windows.size() == (B, adjusted_length, 2 * self.window_size + 1)\n","\n","\n","    embedded_windows = self.embeds(token_windows)\n","    embedded_windows = embedded_windows.view(B, adjusted_length, -1)\n","    layer_1 = self.hidden_layer(embedded_windows)\n","    output = self.output_layer(layer_1)\n","    output = self.probabilities(output)\n","    output = output.view(B, -1)\n","\n","    return output\n","\n"],"metadata":{"id":"fZrzpNo9SPoU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Prepare the data\n","data = list(zip(train_sentences, train_labels))\n","batch_size = 2\n","shuffle = True\n","window_size = 2\n","collate_fn = partial(custom_collate_fn, window_size=window_size, word_to_ix=word_to_ix)\n","\n","# Instantiate a DataLoader\n","loader = DataLoader(data, batch_size=batch_size, shuffle=shuffle, collate_fn=collate_fn)\n","\n","# Initialize a model\n","# It is useful to put all the model hyperparameters in a dictionary\n","model_hyperparameters = {\n","    \"window_size\": 2,\n","    \"embed_dim\": 25,\n","    \"hidden_dim\": 25,\n","    \"freeze_embeddings\": False,\n","}\n","\n","vocab_size = len(word_to_ix)\n","model = WordWindowClassifier(model_hyperparameters, vocab_size)\n","\n","# Define an optimizer\n","learning_rate = 0.01\n","optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n","\n","# Define a loss function, which computes to binary cross entropy loss\n","def loss_function(batch_outputs, batch_labels, batch_lengths):\n","    # Calculate the loss for the whole batch\n","    bceloss = nn.BCELoss()\n","    loss = bceloss(batch_outputs, batch_labels.float())\n","\n","    # Rescale the loss. Remember that we have used lengths to store the\n","    # number of words in each training example\n","    loss = loss / batch_lengths.sum().float()\n","\n","    return loss\n"],"metadata":{"id":"sqADpnjcJfkA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Function that will be called in every epoch\n","def train_epoch(loss_function, optimizer, model, loader):\n","\n","  # Keep track of the total loss for the batch\n","  total_loss = 0\n","  for batch_inputs, batch_labels, batch_lengths in loader:\n","    # Clear the gradients\n","    optimizer.zero_grad()\n","    # Run a forward pass\n","    outputs = model.forward(batch_inputs)\n","\n","    # Compute the batch loss\n","    loss = loss_function(outputs, batch_labels, batch_lengths)\n","    # Calculate the gradients\n","    loss.backward()\n","    # Update the parameteres\n","    optimizer.step()\n","    total_loss += loss.item()\n","\n","  return total_loss\n","\n","\n","# Function containing our main training loop\n","def train(loss_function, optimizer, model, loader, num_epochs=10000):\n","\n","  # Iterate through each epoch and call our train_epoch function\n","  for epoch in range(num_epochs):\n","    epoch_loss = train_epoch(loss_function, optimizer, model, loader)\n","    if epoch % 100 == 0: print(epoch_loss)"],"metadata":{"id":"z-_DTJe7qzGR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["num_epochs = 1000\n","train(loss_function, optimizer, model, loader, num_epochs=num_epochs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-6fvWcESq0pz","executionInfo":{"status":"ok","timestamp":1709376147927,"user_tz":-540,"elapsed":2145,"user":{"displayName":"서나미","userId":"11215307119811085502"}},"outputId":"33e63f55-8c4f-441a-afaf-eef2d2f50e7a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.29740985482931137\n","0.23025033250451088\n","0.18813345581293106\n","0.13635334745049477\n","0.09081567265093327\n","0.07539002597332001\n","0.05844420567154884\n","0.059262012131512165\n","0.040542895905673504\n","0.035953816026449203\n"]}]},{"cell_type":"markdown","source":["### Prediction\n","\n","Let's see how well our model is at making predictions. We can start by creating our test data."],"metadata":{"id":"N_VC6uT7wlV5"}},{"cell_type":"code","source":["# Create test sentences\n","test_corpus = [\"She comes from Paris\"]\n","test_sentences = [s.lower().split() for s in test_corpus]\n","test_labels = [[0, 0, 0, 1]]\n","\n","# Create a test loader\n","test_data = list(zip(test_sentences, test_labels))\n","batch_size = 1\n","shuffle = False\n","window_size = 2\n","collate_fn = partial(custom_collate_fn, window_size=2, word_to_ix=word_to_ix)\n","test_loader = torch.utils.data.DataLoader(test_data,\n","                                           batch_size=1,\n","                                           shuffle=False,\n","                                           collate_fn=collate_fn)"],"metadata":{"id":"qsuNGK6Owiqv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for test_instance, labels, _ in test_loader:\n","  outputs = model.forward(test_instance)\n","  print(labels)\n","  print(outputs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eRlXQN-fwvqn","executionInfo":{"status":"ok","timestamp":1709376213429,"user_tz":-540,"elapsed":286,"user":{"displayName":"서나미","userId":"11215307119811085502"}},"outputId":"9557c68c-836e-4a8c-b4d4-0e7c73086e1a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0, 0, 0, 1]])\n","tensor([[0.0774, 0.0214, 0.1245, 0.9418]], grad_fn=<ViewBackward0>)\n"]}]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}