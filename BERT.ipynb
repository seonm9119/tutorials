{"cells":[{"cell_type":"markdown","source":["#### Setup Codes"],"metadata":{"id":"5TW0q_adF4j9"}},{"cell_type":"code","source":["%load_ext autoreload\n","%autoreload 2"],"metadata":{"id":"eifhyuriF_79","executionInfo":{"status":"ok","timestamp":1715089264980,"user_tz":-540,"elapsed":8,"user":{"displayName":"서나미","userId":"11215307119811085502"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["##### Google Colab Setup\n","we need to run a few commands to set up our environment on Google Colab. If you are running this notebook on a local machine you can skip this section. Run the following cell to mount your Google Drive."],"metadata":{"id":"DpaWhxTmGaxY"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KXQLqRjBGRbP","executionInfo":{"status":"ok","timestamp":1715089284082,"user_tz":-540,"elapsed":19108,"user":{"displayName":"서나미","userId":"11215307119811085502"}},"outputId":"57889413-6832-4269-b4fb-086ca3c729fa"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import os\n","import sys\n","\n","# TODO: Fill in the Google Drive path where you uploaded the assignment\n","# Example: If you create a 'Test' folder and put all the files under 'example' folder, then 'Test/example'\n","# GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = 'Test/example'\n","GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = 'GIT/tutorials/utils/'\n","GOOGLE_DRIVE_PATH = os.path.join('drive', 'My Drive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n","sys.path.append(GOOGLE_DRIVE_PATH)\n","\n","print(os.listdir(GOOGLE_DRIVE_PATH))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ut1DWdsPGTsB","executionInfo":{"status":"ok","timestamp":1715089285205,"user_tz":-540,"elapsed":1127,"user":{"displayName":"서나미","userId":"11215307119811085502"}},"outputId":"10d9fd16-c2cb-462d-9341-7545895a8323"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["['__pycache__', 'for_knn.py', 'linear_classifier.py', 'custom_model_utils', 'Convolutional_Neural_Network', '_utils.py', 'save.py', '_word_processing.py', '_layers.py', 'enc2dec', 'data', 'models', 'colab_utils']\n"]}]},{"cell_type":"markdown","source":["##### NLP Setup Codes"],"metadata":{"id":"LPPMUAuxGjTs"}},{"cell_type":"code","source":["!pip install 'portalocker>=2.0.0'"],"metadata":{"id":"SKrGV_Y2GnVc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install datasets"],"metadata":{"id":"GSWTctydiktV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torchtext\n","import torchdata\n","\n","print(f'torch version: {torch.__version__}')\n","print(f'torchtext version: {torchtext.__version__}')\n","print(f'torchtext data: {torchdata.__version__}')\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qXAjbJs8Gqya","executionInfo":{"status":"ok","timestamp":1715089319590,"user_tz":-540,"elapsed":7659,"user":{"displayName":"서나미","userId":"11215307119811085502"}},"outputId":"7a6b41bf-4bc2-4fd0-fd0b-cba2d1d0eb92"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["torch version: 2.2.1+cu121\n","torchtext version: 0.17.1+cpu\n","torchtext data: 0.7.1\n"]}]},{"cell_type":"markdown","source":["# BERT\n","\n","reference : https://github.com/codertimo/BERT-pytorch\n"],"metadata":{"id":"vkVKxSAlCbJd"}},{"cell_type":"code","source":["import torch.nn as nn\n","\n","# custom package\n","from models.transformer.utils import get_positional_encoding\n","from models.transformer.transformer import EncoderLayer"],"metadata":{"id":"dmowtJ-ZhQ_3","executionInfo":{"status":"ok","timestamp":1715088175697,"user_tz":-540,"elapsed":3662,"user":{"displayName":"서나미","userId":"11215307119811085502"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["## Model Architecture\n","\n","BERT's model architecture is a multi-layer bidirectional Transformer encoder based on the original Transformer. In this work, we denote the number of layers as $L$, the hidden size as $H$, and the number of self-attention heads as $A$. We primarily report results on two model sizes: $BERT_{BASE}(L=12, H=768,A=12, \\text{Total Parameters}=110M)$ and $BERT_{LARGE}(L=24, H=1024,A=16, \\text{Total Parameters}=340M)$. For a given token, its input representation is constructed by summing the corresponding token, segment, and position embeddings.\n","\n","> 논문에선 Transformer의 EncoderLayer를 Transformer Block으로 명명한다. 이름만 다를뿐 Transformer의 EncoderLayer와 같다. BERT모델 구조자체는 심플하다. 논문에서 강조하는 부분은 Masked ML과 Next Sentence Prediction이다. 구현은 이전 Transformer tutorial에서 구현한 모델을 사용했다.\n","\n","> 사실 디테일에 대한 차이가 조금있다. 논문에선 ReLU대신 GELU를 사용한다.\n"],"metadata":{"id":"NsE6vj_U8Sof"}},{"cell_type":"code","source":["class BERT(nn.Module):\n","  def __init__(self, vocab_size,\n","               d_model=768,\n","               n_layers=12,\n","               n_head=12,\n","               dropout=0.1,\n","               PAD_IDX=2,\n","               device='cpu'):\n","      super().__init__()\n","\n","      self.device = device\n","      self.PAD_IDX=PAD_IDX\n","      self.token_embed = nn.Embedding(vocab_size, d_model, padding_idx=PAD_IDX)\n","      self.segment_embed = nn.Embedding(3, d_model, padding_idx=PAD_IDX)\n","      self.position_embed = get_positional_encoding(d_model).to(device)\n","      self.position_embed.require_grad = False\n","\n","      self.dropout = nn.Dropout(dropout)\n","      dim_feedforward = d_model * 4\n","\n","      self.transformer_blocks = nn.ModuleList([\n","          EncoderLayer(d_model, dim_feedforward, n_head, dropout) for _ in range(n_layers)])\n","\n","  def forward(self, x, segment=None):\n","\n","      # pad mask\n","      mask = (x != self.PAD_IDX).unsqueeze(1).repeat(1, x.size(1), 1).unsqueeze(1).to(self.device)\n","\n","      if segment is not None:\n","        embed = self.segment_embed(segment)\n","\n","      embed = self.token_embed(x) + self.position_embed[:,:x.size(1),:]\n","      embed = self.dropout(embed)\n","\n","      for _blocks in self.transformer_blocks:\n","          embed = _blocks(embed, mask)\n","\n","      return embed"],"metadata":{"id":"8FBLKdY0C6iT","executionInfo":{"status":"ok","timestamp":1715088175697,"user_tz":-540,"elapsed":10,"user":{"displayName":"서나미","userId":"11215307119811085502"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["class BertForPretrain(nn.Module):\n","  def __init__(self,vocab_size,\n","               d_model=768,\n","               n_layers=12,\n","               n_head=12,\n","               dropout=0.1,\n","               PAD_IDX=2,\n","               device='cpu'):\n","    super().__init__()\n","\n","    self.bert = BERT(vocab_size, d_model, n_layers, n_head, dropout, PAD_IDX, device)\n","\n","    # Masked LM\n","    self.masked_LM = nn.Linear(d_model, vocab_size)\n","    self.LM_softmax = nn.LogSoftmax(dim=-1)\n","\n","    # Next Sentence Prediction\n","    self.next_sentence_prediction = nn.Linear(d_model, 2)\n","    self.next_softmax = nn.LogSoftmax(dim=-1)\n","\n","  def forward(self, x, segment):\n","\n","    embed = self.bert(x, segment)\n","\n","    mask_lm_output = self.masked_LM(embed)\n","    mask_lm_output = self.LM_softmax(mask_lm_output)\n","\n","    # only use CLS token\n","    next_sent_output = self.next_sentence_prediction(embed)[:, 0]\n","    next_sent_output = self.next_softmax(next_sent_output)\n","\n","\n","    return mask_lm_output, next_sent_output\n"],"metadata":{"id":"XHTzARQBdexw","executionInfo":{"status":"ok","timestamp":1715088175698,"user_tz":-540,"elapsed":10,"user":{"displayName":"서나미","userId":"11215307119811085502"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["## Input/Output Representation\n","\n","We use WordPiece embeddings with a 30,000 token vocabulary.\n","\n","- The first token of every sequence is always a special classification token $\n","([\\text{CLS}])$. The final hidden state corresponding to this token is used as the aggregate sequence representation for classification tasks.\n","- Sentence pairs are packed together into a single sequence. We differentiate the sentences in two ways.\n","  - We seperate them with a special token $([\\text{SEP}])$.\n","  - We add a learned embedding to every token indicating whether it belongs to sentence $\\text{A}$ or $\\text{B}$\n","\n","We denote input embedding as $E$, the final hidden vector of the special $[\\text{CLS}]$ token as $C \\in \\mathbb{R}^H$, and the final hidden vector for the $i^{th}$ input token as $T^i \\in \\mathbb{R}^H$."],"metadata":{"id":"DheNXlN5KQo5"}},{"cell_type":"markdown","source":["- BERT를 학습하는데 가장 중요한 부분이다. 모델자체를 간단하지만, 모델을 학습하는데 어떻게 representation하는지에 대한 부분이 중요하게 다뤄진다.\n","\n","- 마스킹 모델링은 무작위로 단어를 마스킹하는데, 이러한 방식이 단어를 양방향으로 학습할수 있다고 강조한다."],"metadata":{"id":"Vhx_jwcEU6CC"}},{"cell_type":"code","source":["import random\n","from torch.nn.utils.rnn import pad_sequence\n","from torch.utils.data import Dataset, DataLoader\n","\n","# custom packages\n","from data.wikitext2 import load_WikiText2\n","from data.word_piece_vocab import build_wordpiece_vocab"],"metadata":{"id":"CIYmlzMoyw3A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["datasets, _, _ = load_WikiText2()\n","tokenizer, vocab = build_wordpiece_vocab(datasets, vocab_file=None, vocab_size=30000)\n","\n","print(f\"size of vocab :{len(vocab)}\")\n","print(f\"size of datasets : {len(datasets)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RHRc6WpAvQPD","executionInfo":{"status":"ok","timestamp":1715089474193,"user_tz":-540,"elapsed":15147,"user":{"displayName":"서나미","userId":"11215307119811085502"}},"outputId":"23807ced-89ef-4ca2-809e-72ae387304ab"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["size of vocab :30000\n","size of datasets : 79603\n"]}]},{"cell_type":"code","source":["class BERTDataset(Dataset):\n","  def __init__(self, corpus, tokenizer, vocab, max_len=512,\n","               CLS='[CLS]', SEP='[SEP]', MASK='[MASK]', PAD='[PAD]'):\n","\n","    self.CLS, self.SEP = CLS, SEP\n","    self.MASK, self.PAD = MASK, PAD\n","\n","    self.max_len = max_len\n","    self.corpus = corpus\n","    self.corpus_index = list(range(len(corpus)))\n","    self.tokenizer = tokenizer\n","    self.vocab = vocab\n","\n","  def __len__(self,):\n","    return len(self.corpus) - 1\n","\n","  def __getitem__(self, idx):\n","\n","    # random sentence\n","    t1, t2, is_next_label = self.get_random_sent(idx)\n","\n","    # random masking\n","    masked_t1, t1_label = self.random_masking(t1)\n","    masked_t2, t2_label = self.random_masking(t2)\n","\n","\n","    # segment_label\n","    segment_label = ([1 for _ in range(len(masked_t1) + 2)] + [2 for _ in range(len(masked_t2) + 1)])[:self.max_len]\n","    input = ([self.CLS] + masked_t1 + [self.SEP] + masked_t2 + [self.SEP])[:self.max_len]\n","    label = ([self.CLS] + t1_label + [self.SEP] + t2_label + [self.SEP])[:self.max_len]\n","\n","\n","    # print(f\"Is Next label? {is_next_label}\\n\")\n","    # print(f\"Input Token: {input}\")\n","    # print(f\"Label Token: {label}\")\n","    # print(f\"Segment Label: {segment_label}\")\n","\n","\n","    # convert token to idx\n","    input = self.convert_token_to_idx(input)\n","    label = self.convert_token_to_idx(label)\n","    segment_label = torch.tensor(segment_label, dtype=torch.int64)\n","\n","    return input, label, segment_label, is_next_label\n","\n","\n","  def get_random_sent(self, idx):\n","\n","    t1 = self.corpus[idx]\n","    t2 = self.corpus[idx + 1]\n","\n","    #print(f\"Sentence 1: {t1}\")\n","\n","    if random.random() > 0.5:\n","      #print(f\"Sentence 2: {t2}\")\n","      return t1, t2, True\n","\n","    else:\n","      corpus_index = self.corpus_index.copy()\n","      corpus_index.remove(idx + 1)\n","      rnd_idx = random.choice(corpus_index)\n","\n","      t2 = self.corpus[rnd_idx]\n","      #print(f\"Sentence 2: {t2}\")\n","      return t1, t2, False\n","\n","  def random_masking(self, sentence):\n","\n","    # convert sentence to tokens\n","    tokens = self.tokenizer(sentence)\n","    new_token = tokens.copy()\n","    label = []\n","\n","    # mask 15% of all tokens in each sequence at random\n","    for idx in range(len(tokens)):\n","      label.append(self.PAD)\n","\n","      if random.random() < 0.15:\n","        prob = random.random()\n","\n","        # (1) the [MASK] token 80% of the time\n","        if prob < 0.8:\n","          new_token[idx] = self.MASK\n","          label[idx] = tokens[idx]\n","\n","        # (2) a random token 10% of the time\n","        elif prob < 0.9:\n","          rnd_idx = random.randint(0, len(self.vocab)-1)\n","          new_token[idx] = self.vocab.itos[rnd_idx]\n","\n","    return new_token, label\n","\n","  def convert_token_to_idx(self, tokens):\n","    tokens = self.vocab.convert_tokens_to_indices(tokens)\n","    return torch.tensor(tokens, dtype=torch.int64)"],"metadata":{"id":"uENOqvAqy6k9","executionInfo":{"status":"ok","timestamp":1715089496029,"user_tz":-540,"elapsed":557,"user":{"displayName":"서나미","userId":"11215307119811085502"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["class BERTCollate(object):\n","    def __init__(self, PAD_IDX=2, batch_first=True):\n","        self.PAD_IDX = PAD_IDX\n","        self.batch_first = batch_first\n","\n","    def __call__(self, batch):\n","        inputs, labels, segments, nexts = zip(*batch)\n","        inputs = pad_sequence(inputs, padding_value=self.PAD_IDX, batch_first=self.batch_first)\n","        labels = pad_sequence(labels, padding_value=self.PAD_IDX, batch_first=self.batch_first)\n","        segments = pad_sequence(segments, padding_value=self.PAD_IDX, batch_first=self.batch_first)\n","        return inputs, labels, segments, torch.tensor(nexts, dtype=torch.int64)\n"],"metadata":{"id":"Gg7qZYX9y9aJ","executionInfo":{"status":"ok","timestamp":1715089498671,"user_tz":-540,"elapsed":7,"user":{"displayName":"서나미","userId":"11215307119811085502"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["## Sanity Check"],"metadata":{"id":"BBBtHFEq837d"}},{"cell_type":"code","source":["sample_data = ['Once upon a time, a young boy moved to a small village.',\n","               'He always had a smile on his face as he made friends around.',\n","               'One day, a large snake appeared near the village, causing fear and panic among the villagers.',\n","               'They spread rumors and everyone felt uneasy.']"],"metadata":{"id":"K2POIuKhwcZd","executionInfo":{"status":"ok","timestamp":1715089501181,"user_tz":-540,"elapsed":6,"user":{"displayName":"서나미","userId":"11215307119811085502"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["sample_dataset = BERTDataset(sample_data, tokenizer, vocab)\n","collate_fn = BERTCollate(PAD_IDX=vocab.PAD_IDX)\n","loader = DataLoader(sample_dataset, batch_size=1, collate_fn=collate_fn, shuffle=True)"],"metadata":{"id":"b_EejzUXwewz","executionInfo":{"status":"ok","timestamp":1715089502345,"user_tz":-540,"elapsed":5,"user":{"displayName":"서나미","userId":"11215307119811085502"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["input, label, segment_label, is_next_label = next(iter(loader))\n","\n","print(f\"\\nConverted Input: {input.tolist()}\")\n","print(f\"Converted Label: {label.tolist()}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yjCahc3qwgib","executionInfo":{"status":"ok","timestamp":1715089510351,"user_tz":-540,"elapsed":297,"user":{"displayName":"서나미","userId":"11215307119811085502"}},"outputId":"5e08a7ec-d9a1-4a48-d5c5-7371202c0845"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Sentence 1: He always had a smile on his face as he made friends around.\n","Sentence 2: Once upon a time, a young boy moved to a small village.\n","Is Next label? False\n","\n","Input Token: ['[CLS]', '[UNK]', 'always', 'had', 'a', 'smile', '[MASK]', 'his', 'face', 'as', 'he', 'made', 'friends', '[UNK]', '[SEP]', '[MASK]', 'upon', 'a', '[UNK]', 'a', 'young', 'boy', 'moved', 'to', 'a', 'small', '[UNK]', '[SEP]']\n","Label Token: ['[CLS]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'on', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[SEP]', '[UNK]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[SEP]']\n","Segment Label: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n","\n","Converted Input: [[0, 1, 6606, 755, 25566, 3515, 4, 26006, 28062, 27652, 2630, 19327, 25947, 1, 3, 4, 15585, 25566, 1, 25566, 12737, 25347, 21482, 14493, 25566, 7413, 1, 3]]\n","Converted Label: [[0, 2, 2, 2, 2, 2, 14871, 2, 2, 2, 2, 2, 2, 2, 3, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3]]\n"]}]},{"cell_type":"markdown","source":["# Sanity check"],"metadata":{"id":"k8G9-yLLUnEw"}},{"cell_type":"code","source":["import torch.optim as optim\n","\n","# custom package\n","import colab_utils.bert as utils"],"metadata":{"id":"8RSphKbVc7l3","executionInfo":{"status":"ok","timestamp":1715088214314,"user_tz":-540,"elapsed":1733,"user":{"displayName":"서나미","userId":"11215307119811085502"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["train_datasets = BERTDataset(datasets, tokenizer, vocab, max_len=64)\n","collate_fn = BERTCollate(PAD_IDX=vocab.PAD_IDX)\n","data_loader = DataLoader(train_datasets, batch_size=128, collate_fn=collate_fn, shuffle=True)"],"metadata":{"id":"pVY1YmN1zCxW","executionInfo":{"status":"ok","timestamp":1715088215456,"user_tz":-540,"elapsed":1148,"user":{"displayName":"서나미","userId":"11215307119811085502"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["model = BertForPretrain(vocab_size=len(vocab),\n","                        d_model=768,\n","                        n_layers=12,\n","                        n_head=12,\n","                        dropout=0.1,\n","                        PAD_IDX=vocab.PAD_IDX,\n","                        device=device)\n","\n","criterion = nn.CrossEntropyLoss(ignore_index=vocab.PAD_IDX)\n","optimizer = optim.Adam(model.parameters(), lr=1e-4, betas=(0.9, 0.999), weight_decay=0.01)"],"metadata":{"id":"GV2f9Zg_z8x5","executionInfo":{"status":"ok","timestamp":1715088217360,"user_tz":-540,"elapsed":1908,"user":{"displayName":"서나미","userId":"11215307119811085502"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["model = model.to(device)\n","\n","inputs, labels, segments, nexts = next(iter(data_loader))\n","inputs = inputs.to(device)\n","labels = labels.to(device)\n","segments = segments.to(device)\n","nexts = nexts.to(device)\n","\n","optimizer.zero_grad()\n","mask_lm_output, next_sent_output = model(inputs, segments)\n","mask_loss = criterion(mask_lm_output.transpose(2,1), labels)\n","next_loss = criterion(next_sent_output, nexts)\n","loss = mask_loss + next_loss\n","\n","print(f\"loss : {loss.item()}\")\n","\n","loss.backward()\n","optimizer.step()"],"metadata":{"id":"N1kfV_JsUo3Z","executionInfo":{"status":"ok","timestamp":1715088219521,"user_tz":-540,"elapsed":2171,"user":{"displayName":"서나미","userId":"11215307119811085502"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"19153b02-6501-4776-f5ed-f2a4cd7c6919"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["loss : 11.127464294433594\n"]}]},{"cell_type":"markdown","source":["# Pre-train Net"],"metadata":{"id":"xvHbeVoAok7u"}},{"cell_type":"code","source":["history = utils.runner(model, criterion, optimizer, data_loader, 3, mode='train')\n","torch.save(model.bert.state_dict(), 'bert.pth')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ik6QrN_rqPm3","executionInfo":{"status":"ok","timestamp":1715089163147,"user_tz":-540,"elapsed":943645,"user":{"displayName":"서나미","userId":"11215307119811085502"}},"outputId":"08fa96d6-a5a7-4e5d-c8a1-16c739efe9bb"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Train using cuda\n","Epoch [1/3]          time: 0:05:12          Loss: 5.7094          \n","Epoch [2/3]          time: 0:05:12          Loss: 5.5664          \n","Epoch [3/3]          time: 0:05:12          Loss: 5.5658          \n","\n","Finished Training\n","Toral Training Time: 0:15:37\n"]}]},{"cell_type":"markdown","source":["# Finetune"],"metadata":{"id":"LKV9duMkh53m"}},{"cell_type":"code","source":["from torch.utils.data import DataLoader, TensorDataset\n","\n","# custom package\n","from data.word_processing import build_transform\n","from data.sst_2 import load_SST2, build_SST2_vocab, SST2Collate"],"metadata":{"id":"r-8htLCajFnv","executionInfo":{"status":"ok","timestamp":1715089164067,"user_tz":-540,"elapsed":955,"user":{"displayName":"서나미","userId":"11215307119811085502"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["_, dev_datasets, _ = load_SST2(root='.')\n","collate_fn = SST2Collate(build_transform(tokenizer, vocab),\n","                         PAD_IDX=vocab.PAD_IDX,\n","                         batch_first=True)\n","\n","dev_dataloader = DataLoader(dev_datasets, batch_size=32, collate_fn=collate_fn)\n","\n","print(f\"size of dataset :{len(dev_datasets)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9pMJdOGqi1sv","executionInfo":{"status":"ok","timestamp":1715089165143,"user_tz":-540,"elapsed":1082,"user":{"displayName":"서나미","userId":"11215307119811085502"}},"outputId":"5b5ebf64-8f25-47ae-8304-d2b34274d4d0"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["size of dataset :872\n"]}]},{"cell_type":"code","source":["bert = BERT(vocab_size=len(vocab),\n","            d_model=768,\n","            n_layers=12,\n","            n_head=12,\n","            dropout=0.1,\n","            device=device)\n","\n","bert.load_state_dict(torch.load(\"bert.pth\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mwr1oS9Vrx7A","executionInfo":{"status":"ok","timestamp":1715089171588,"user_tz":-540,"elapsed":2205,"user":{"displayName":"서나미","userId":"11215307119811085502"}},"outputId":"4394ad6f-1a8c-4b59-9e57-cbdc1d13039d"},"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":["classifier = nn.Linear(768, 2)\n","model = nn.Sequential(bert, classifier)\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=5e-5)"],"metadata":{"id":"SzkZy8H-i-oY","executionInfo":{"status":"ok","timestamp":1715089174737,"user_tz":-540,"elapsed":6,"user":{"displayName":"서나미","userId":"11215307119811085502"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["history = utils.runner(model, criterion, optimizer, dev_dataloader, 3, mode='finetune')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rNkLSmrSpXqo","executionInfo":{"status":"ok","timestamp":1715089185796,"user_tz":-540,"elapsed":8787,"user":{"displayName":"서나미","userId":"11215307119811085502"}},"outputId":"334a184e-4b74-41aa-aa73-0e8b58dec124"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["Train using cuda\n","Epoch [1/3]          time: 0:00:02          Loss: 0.7122          ACC: 48.62%          \n","Epoch [2/3]          time: 0:00:02          Loss: 0.6965          ACC: 50.23%          \n","Epoch [3/3]          time: 0:00:02          Loss: 0.6987          ACC: 50.46%          \n","\n","Finished Training\n","Toral Training Time: 0:00:07\n"]}]},{"cell_type":"markdown","source":["### Use Transform Library\n","\n","BERT를 직접 학습시키는데는 많은 시간과 GPU가 필요하다. 코랩에서 pre-train을 진행하기에는 어려움이 있다. BERT의 pre-train방식과 간단한 finetuning을 통해 작동방식을 알아보았고, 실제로 pre-train된 모델을 불러와 finetune하는 방법은 아래를 참조바란다.\n","\n","> Hugging Face에서 해당 API에 대한 정보를 얻을수다.\n","\n","- Hugging Face :https://huggingface.co/docs/tokenizers/api/tokenizer\n","- git hub :https://github.com/huggingface/transformers/blob/main/src/transformers/models/bert/tokenization_bert.py"],"metadata":{"id":"uMb0H6yAlnyE"}},{"cell_type":"code","source":["from transformers import BertTokenizer, BertForSequenceClassification"],"metadata":{"id":"lmE7DXWHmEg4","executionInfo":{"status":"ok","timestamp":1715089191232,"user_tz":-540,"elapsed":562,"user":{"displayName":"서나미","userId":"11215307119811085502"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["inputs, labels = zip(*dev_datasets)\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","tokenized_texts = tokenizer(inputs, padding=True, truncation=True, return_tensors=\"pt\")\n","\n","dataset = TensorDataset(tokenized_texts[\"input_ids\"], tokenized_texts[\"attention_mask\"], torch.tensor(labels))\n","train_dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n","\n","model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n","optimizer = optim.Adam(model.parameters(), lr=5e-5)"],"metadata":{"id":"kLE_XM_7mKpB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["history = utils.runner(model, None, optimizer, train_dataloader, 3, mode='finetune', custom=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Dsly60J7mbn0","executionInfo":{"status":"ok","timestamp":1715089210330,"user_tz":-540,"elapsed":7939,"user":{"displayName":"서나미","userId":"11215307119811085502"}},"outputId":"2cdf70d8-0bb3-4619-ab10-8b20d30a7961"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["Train using cuda\n","Epoch [1/3]          time: 0:00:02          Loss: 0.4926          ACC: 74.77%          \n","Epoch [2/3]          time: 0:00:02          Loss: 0.1228          ACC: 95.99%          \n","Epoch [3/3]          time: 0:00:02          Loss: 0.0397          ACC: 99.08%          \n","\n","Finished Training\n","Toral Training Time: 0:00:08\n"]}]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.16"}},"nbformat":4,"nbformat_minor":0}