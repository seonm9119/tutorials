{"cells":[{"cell_type":"markdown","metadata":{"id":"5TW0q_adF4j9"},"source":["#### Setup Codes"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"eifhyuriF_79","executionInfo":{"status":"ok","timestamp":1717297553840,"user_tz":-540,"elapsed":16,"user":{"displayName":"서나미","userId":"11215307119811085502"}}},"outputs":[],"source":["%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"markdown","metadata":{"id":"DpaWhxTmGaxY"},"source":["##### Google Colab Setup\n","we need to run a few commands to set up our environment on Google Colab. If you are running this notebook on a local machine you can skip this section. Run the following cell to mount your Google Drive."]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18987,"status":"ok","timestamp":1717297572813,"user":{"displayName":"서나미","userId":"11215307119811085502"},"user_tz":-540},"id":"KXQLqRjBGRbP","outputId":"a2b8ab73-d510-4847-e94b-2656490f8168"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1274,"status":"ok","timestamp":1717297574078,"user":{"displayName":"서나미","userId":"11215307119811085502"},"user_tz":-540},"id":"Ut1DWdsPGTsB","outputId":"2073e676-7f54-4060-9f71-063f64411703"},"outputs":[{"output_type":"stream","name":"stdout","text":["['__pycache__', 'for_knn.py', 'linear_classifier.py', 'custom_model_utils', 'Convolutional_Neural_Network', '_utils.py', 'save.py', '_word_processing.py', '_layers.py', 'enc2dec', 'data', 'models', 'colab_utils', 'visualize.py']\n"]}],"source":["import os\n","import sys\n","\n","# TODO: Fill in the Google Drive path where you uploaded the assignment\n","# Example: If you create a 'Test' folder and put all the files under 'example' folder, then 'Test/example'\n","# GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = 'Test/example'\n","GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = 'GIT/tutorials/utils/'\n","GOOGLE_DRIVE_PATH = os.path.join('drive', 'My Drive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n","sys.path.append(GOOGLE_DRIVE_PATH)\n","\n","print(os.listdir(GOOGLE_DRIVE_PATH))"]},{"cell_type":"markdown","metadata":{"id":"W5rT-iMksjqG"},"source":["### Load PASCAL VOC 2007 data"]},{"cell_type":"markdown","metadata":{"id":"9cNPQoBgs4E2"},"source":["PASCAL VOC 2007 download info : https://pytorch.org/vision/main/generated/torchvision.datasets.VOCDetection.html"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"VPOUxYqav8xV","executionInfo":{"status":"ok","timestamp":1717297584432,"user_tz":-540,"elapsed":10357,"user":{"displayName":"서나미","userId":"11215307119811085502"}}},"outputs":[],"source":["from torch.utils.data import DataLoader\n","\n","# custom packages\n","from data.pascal_voc import PASCALVOC, inverse_image\n","from data.cv_utils import detection_visualizer"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26094,"status":"ok","timestamp":1717297610511,"user":{"displayName":"서나미","userId":"11215307119811085502"},"user_tz":-540},"id":"Ai5LG4__s0ue","outputId":"f90b3fa5-7020-4474-babe-eb7a1d31b624"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar to ./VOCtrainval_06-Nov-2007.tar\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 460032000/460032000 [00:13<00:00, 34776579.86it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./VOCtrainval_06-Nov-2007.tar to .\n","Using downloaded and verified file: ./VOCtrainval_06-Nov-2007.tar\n","Extracting ./VOCtrainval_06-Nov-2007.tar to .\n"]}],"source":["train_dataset = PASCALVOC(root='.', year='2007', split='train', download=True, image_size=224)\n","val_dataset = PASCALVOC(root='.', year='2007', split='val', download=True, image_size=224)"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"fyqZGDM3E3tw","executionInfo":{"status":"ok","timestamp":1717297610512,"user_tz":-540,"elapsed":7,"user":{"displayName":"서나미","userId":"11215307119811085502"}}},"outputs":[],"source":["import multiprocessing\n","train_loader = DataLoader(train_dataset, batch_size=32, pin_memory=True, num_workers=multiprocessing.cpu_count())\n","val_loader = DataLoader(val_dataset, batch_size=1, pin_memory=True, num_workers=multiprocessing.cpu_count())"]},{"cell_type":"markdown","metadata":{"id":"kdRYG7gELhFM"},"source":[]},{"cell_type":"markdown","metadata":{"id":"EGcy6MMtNoFp"},"source":["## Faster R-CNN (2)"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"or_is1pNd7JV","executionInfo":{"status":"ok","timestamp":1717297682168,"user_tz":-540,"elapsed":7,"user":{"displayName":"서나미","userId":"11215307119811085502"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","# custom packages\n","import colab_utils.object_detection as utils\n","import data.cv_utils as cv_utils"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1717297610955,"user":{"displayName":"서나미","userId":"11215307119811085502"},"user_tz":-540},"id":"j6hlZ1QRC-RK"},"outputs":[],"source":["def calculate_iou(boxes1, boxes2):\n","\n","    gt = boxes1.repeat(1, boxes2.shape[0]).reshape(-1,4)\n","    anchor = boxes2.repeat(boxes1.shape[0], 1)\n","\n","    # Calculate area of each box\n","    anchor_area = (anchor[:,2] - anchor[:,0]) * (anchor[:,3] - anchor[:,1])\n","    gt_area = (gt[:,2] - gt[:,0]) * (gt[:,3] - gt[:,1])\n","\n","    # Calculate intersection coordinates\n","    intersection_x1 = torch.max(anchor[:,0], gt[:,0])\n","    intersection_y1 = torch.max(anchor[:,1], gt[:,1])\n","    intersection_x2 = torch.min(anchor[:,2], gt[:,2])\n","    intersection_y2 = torch.min(anchor[:,3], gt[:,3])\n","\n","    # Calculate intersection area\n","    intersection_area = torch.max(torch.tensor(0), intersection_x2 - intersection_x1) * torch.max(torch.tensor(0), intersection_y2 - intersection_y1)\n","\n","    # Calculate union area\n","    union_area = anchor_area + gt_area - intersection_area\n","\n","    # Calculate IOU\n","    iou = (intersection_area / union_area).reshape(boxes1.shape[0], -1).T\n","    return iou"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":268,"status":"ok","timestamp":1717297611216,"user":{"displayName":"서나미","userId":"11215307119811085502"},"user_tz":-540},"id":"OzeiPlZuBcI5"},"outputs":[],"source":["boxes1 = torch.Tensor([[10, 10, 90, 90], [60, 60, 80, 80], [30, 30, 70, 70]])\n","boxes2 = torch.Tensor([[10, 10, 90, 90], [20, 20, 40, 40], [60, 60, 80, 80]])\n","\n","result_iou = calculate_iou(boxes1, boxes2)"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1717297611217,"user":{"displayName":"서나미","userId":"11215307119811085502"},"user_tz":-540},"id":"6ClLndT9AFhB"},"outputs":[],"source":["def match_anchors_to_gt(anchor_boxes, gt_boxes, iou_thresholds=(0.3, 0.6)):\n","\n","    # Filter empty GT boxes:\n","    gt_boxes = gt_boxes[gt_boxes[:, 4] != -1]\n","\n","    # If no GT boxes are available, match all anchors to background and return.\n","    if len(gt_boxes) == 0:\n","        fake_boxes = torch.zeros_like(anchor_boxes) - 1\n","        fake_class = torch.zeros_like(anchor_boxes[:, [0]]) - 1\n","        return torch.cat([fake_boxes, fake_class], dim=2)\n","\n","    # Match matrix => pairwise IoU of anchors (rows) and GT boxes (columns).\n","    # STUDENTS: This matching depends on your IoU implementation.\n","    match_matrix = calculate_iou(gt_boxes[:, :4], anchor_boxes)\n","\n","    # Find matched ground-truth instance per anchor:\n","    match_quality, matched_idxs = match_matrix.max(dim=1)\n","    matched_gt_boxes = gt_boxes[matched_idxs]\n","\n","    # Set boxes with low IoU threshold to background (-1).\n","    matched_gt_boxes[match_quality <= iou_thresholds[0]] = -1\n","\n","    # Set remaining boxes to neutral (-1e8).\n","    neutral_idxs = (match_quality > iou_thresholds[0]) & (match_quality < iou_thresholds[1])\n","    matched_gt_boxes[neutral_idxs] = -1e8\n","    return matched_gt_boxes\n"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":4609,"status":"ok","timestamp":1717297615821,"user":{"displayName":"서나미","userId":"11215307119811085502"},"user_tz":-540},"id":"dnh8bFgqD4hU","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1HhT-gNbpv1TV7OZDTy_5dMtFVBvnpHOo"},"outputId":"ee608233-2a0f-479e-b00f-709890b69b32"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["_, image, gt_boxes = train_dataset[0]\n","anchors = utils.generate_anchors(stride=16, ratios=[0.5, 1.0, 2.0], scales=[8])\n","matched_gt_boxes = match_anchors_to_gt(anchors, gt_boxes, iou_thresholds=(0.3, 0.6))\n","\n","fg_idxs_p4 = (matched_gt_boxes[:, 4] > 0).nonzero()\n","\n","for fg_idx in fg_idxs_p4:\n","  dummy_vis_boxes = [anchors[fg_idx][0], matched_gt_boxes[fg_idx][0]]\n","\n","  print(f\"Unlabeled red box is positive anchor: {anchors[fg_idx][0]}\")\n","  cv_utils.detection_visualizer(\n","      inverse_image(image),\n","      train_dataset.idx_to_class,\n","      bbox=dummy_vis_boxes,\n","    )"]},{"cell_type":"code","source":["def sample_training(gt_boxes, num_samples, fg_fraction):\n","\n","    foreground = (gt_boxes[:, 4] >= 0).nonzero().squeeze(1)\n","    background = (gt_boxes[:, 4] == -1).nonzero().squeeze(1)\n","\n","    # Protect against not enough foreground examples.\n","    num_fg = min(int(num_samples * fg_fraction), foreground.numel())\n","    num_bg = num_samples - num_fg\n","\n","    # Randomly select positive and negative examples.\n","    perm1 = torch.randperm(foreground.numel(), device=foreground.device)[:num_fg]\n","    perm2 = torch.randperm(background.numel(), device=background.device)[:num_bg]\n","\n","    fg_idx = foreground[perm1]\n","    bg_idx = background[perm2]\n","    return fg_idx, bg_idx"],"metadata":{"id":"ZFhRwTLDZmDj","executionInfo":{"status":"ok","timestamp":1717297615822,"user_tz":-540,"elapsed":5,"user":{"displayName":"서나미","userId":"11215307119811085502"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","execution_count":13,"metadata":{"id":"KzeP4pXldxsW","executionInfo":{"status":"ok","timestamp":1717297615822,"user_tz":-540,"elapsed":4,"user":{"displayName":"서나미","userId":"11215307119811085502"}}},"outputs":[],"source":["class RegionProposalNetwork(nn.Module):\n","    def __init__(self, feat_dim,\n","                 out_dim=512,\n","                 num_anchors=3,\n","                 img_size=(224, 224),\n","                 stride=16,\n","                 ratios=[0.5, 1.0, 2.0],\n","                 scales=[8],\n","                 pre_nms_topN=400,\n","                 post_nms_topN=100,\n","                 thresh=0.7,\n","                 iou_thresholds=(0.3, 0.6)):\n","        super().__init__()\n","\n","        self.prediction = utils.PredictionNetwork(feat_dim,\n","                                                  out_dim=512,\n","                                                  num_anchors=3)\n","\n","        self.proposal = utils.ProposalLayer(img_size=(224, 224),\n","                                            stride=stride,\n","                                            ratios=[0.5, 1.0, 2.0],\n","                                            scales=[8],\n","                                            pre_nms_topN=400,\n","                                            post_nms_topN=100,\n","                                            thresh=0.7)\n","\n","        self.batch_size_per_image = 1\n","        self.iou_thresholds = iou_thresholds\n","\n","    def forward(self, base_feat, gt_boxes):\n","\n","        batch_size = base_feat.size(0)\n","\n","        # 1. predict deltas, cls_score\n","        bbox_deltas, scores = self.prediction(base_feat)\n","        proposals, anchors = self.proposal(scores, bbox_deltas)\n","        matched_gt = utils.get_match_anchor(anchors, gt_boxes, self.iou_thresholds)\n","\n","        # collapse 'batch_size'\n","        bbox_deltas = bbox_deltas.reshape(-1, 4)\n","        scores = scores.reshape(-1)\n","        anchors = anchors.reshape(-1, 4)\n","        matched_gt = matched_gt.reshape(-1, 5)\n","\n","\n","        # 2. sample\n","        fg_idx, bg_idx = utils.sample_training(matched_gt, self.batch_size_per_image * batch_size, fg_fraction=0.5)\n","        idx = torch.cat((fg_idx, bg_idx), 0)\n","\n","        sampled_gt_fg = torch.ones_like(fg_idx)\n","        sampled_gt_bg = torch.zeros_like(bg_idx)\n","        sampled_gt_objectness = torch.cat((sampled_gt_fg, sampled_gt_bg), 0).float()\n","\n","\n","        # Step 2: Compute GT targets for box regression\n","        sampled_anchor = anchors[idx]\n","        sampled_matched_gt = matched_gt[idx]\n","        sampled_scores = scores[idx]\n","        sampled_bbox_deltas = bbox_deltas[idx]\n","\n","        sampled_gt_deltas = utils.get_deltas_from_anchors(sampled_anchor, sampled_matched_gt)\n","\n","\n","        # Step 3: Calculate objectness and box reg losses per sampled anchor\n","        loss_box = F.l1_loss(sampled_bbox_deltas, sampled_gt_deltas, reduction=\"none\")\n","        loss_box[sampled_gt_deltas == -1e8] *= 0\n","\n","        loss_obj = F.binary_cross_entropy_with_logits(sampled_scores, sampled_gt_objectness, reduction=\"none\")\n","\n","        total_batch_size = self.batch_size_per_image * batch_size\n","        loss_obj = loss_obj.sum() / total_batch_size\n","        loss_box = loss_box.sum() / total_batch_size\n","\n","        return proposals, loss_obj, loss_box"]}],"metadata":{"colab":{"collapsed_sections":["5TW0q_adF4j9","W5rT-iMksjqG"],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.16"}},"nbformat":4,"nbformat_minor":0}