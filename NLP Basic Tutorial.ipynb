{"cells":[{"cell_type":"markdown","source":["## *NLP Basic Tutorial*\n","#### referenced by CS224N: PyTorch Tutorial (Winter '21)\n","\n","Our goal will be to train a model that will find the words in a sentence corresponding to a `LOCATION`, which will be always of span `1` (meaning that `San Fransisco` won't be recognized as a `LOCATION`). Our task is called `Word Window Classification` for a reason.\n","\n","\n","\n","> 해당노트는 `Window Classification`을 설계하는데 필요한 기본적인 `components`에 대해 설명한다.\n","\n","\n","\n"],"metadata":{"id":"N6uLiuUhIEn1"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn"],"metadata":{"id":"O-8b4F8EDod9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Our raw data, which consists of sentences\n","corpus = [\n","          \"We always come to Paris\",\n","          \"The professor is from Australia\",\n","          \"I live in Stanford\",\n","          \"He comes from Taiwan\",\n","          \"The capital of Turkey is Ankara\"\n","         ]"],"metadata":{"id":"6Jz45HyQ8lyu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Preprocessing\n","\n","텍스트 데이터사용하여 모델을 학습할 때에는 보통 몇 가지의 전처리 단계를 적용한다. 다음은 `text preprocessing`의 몇 가지 예이다.\n","\n","* **Tokenization**: 문장을 단어로 토큰화.\n","* **Lowercasing**: 소문자로 변경.\n","* **Noise removal**: 특수문자(쉼표, 마침표 등) 제거.\n","* **Stop words removal**: 일반적으로 사용하는 단어를 제거.\n","\n","어떤 전처리 단계가 필요한지는 당면한 작업에 의해 달라질 수 있다. 이번 작업에서는 단어를 소문자로 변경하고 문장을 단어로 토큰화한다."],"metadata":{"id":"v9E-XRjcLyKm"}},{"cell_type":"code","source":["# lowercase the letters and then tokenize the words.\n","train_sentences = [sent.lower().split() for sent in corpus]\n","train_sentences"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d6rTFnEICCcu","executionInfo":{"status":"ok","timestamp":1709373493592,"user_tz":-540,"elapsed":24,"user":{"displayName":"서나미","userId":"11215307119811085502"}},"outputId":"11f6d572-d325-41a6-8f76-f47dc93ab18b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[['we', 'always', 'come', 'to', 'paris'],\n"," ['the', 'professor', 'is', 'from', 'australia'],\n"," ['i', 'live', 'in', 'stanford'],\n"," ['he', 'comes', 'from', 'taiwan'],\n"," ['the', 'capital', 'of', 'turkey', 'is', 'ankara']]"]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","source":["#### Converting Words to Embeddings\n","\n","Imagine that we have an embedding lookup table `E`, where each row corresponds to an embedding. That is, each word in our vocabulary would have a corresponding embedding row `i` in this table. Whenever we want to find an embedding for a word, we will follow these steps:\n","1. Find the corresponding index `i` of the word in the embedding table: `word->index`.\n","2. Index into the embedding table and get the embedding: `index->embedding`.\n","\n","\n","> 모델을 학습시키기 위해 우리가 가지고 있는 `train_sentences`를 벡터로의 변환이 필요하다. (변환된 벡터를 `embedding`이라고 한다) 이러한 `embedding`을 만들기 위해 선행되어야 하는 작업은  다음과 같다.\n","\n","\n","1. Find all the unique words in our corpus.\n","  - vocaborary 구축\n","2. Assign an index to each.\n","  - word_to_idx, idx_to_word 구축"],"metadata":{"id":"dPndvrgZN8ZL"}},{"cell_type":"markdown","source":["#### 1. Find all the unique words in our corpus."],"metadata":{"id":"lRkGspFnUv27"}},{"cell_type":"code","source":["# Find all the unique words in our corpus\n","vocabulary = set(w for s in train_sentences for w in s)\n","\n","# Add the unknown token to our vocabulary\n","vocabulary.add(\"<unk>\")\n","\n","# Add the <pad> token to our vocabulary\n","vocabulary.add(\"<pad>\")\n","\n","vocabulary"],"metadata":{"id":"V2FSDDHlCm7J","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709373493592,"user_tz":-540,"elapsed":22,"user":{"displayName":"서나미","userId":"11215307119811085502"}},"outputId":"778c9938-e51e-4d42-b9ee-e03b1280c986"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'<pad>',\n"," '<unk>',\n"," 'always',\n"," 'ankara',\n"," 'australia',\n"," 'capital',\n"," 'come',\n"," 'comes',\n"," 'from',\n"," 'he',\n"," 'i',\n"," 'in',\n"," 'is',\n"," 'live',\n"," 'of',\n"," 'paris',\n"," 'professor',\n"," 'stanford',\n"," 'taiwan',\n"," 'the',\n"," 'to',\n"," 'turkey',\n"," 'we'}"]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","source":["During the test time, we can see words that are not contained in our vocabulary. We will add this special token to our vocabulary.\n","\n","> `<unk>` : `vocabulary`에 없는 단어가 나타났을 경우 처리하기 위해 필요한 `token`.\n","\n","Word windows allow our model to consider the surrounding `+N` or `-N` words of each word when making a prediction. In our earlier example for `Paris`, if we have a window size of 1, that means our model will look at the words that come immediately before and after `Paris`, which are `to`, and, well, nothing. Now, this raises another issue. `Paris` is at the end of our sentence, so there isn't another word following it. Remember that we define the input dimensions of our `PyTorch` models when we are initializing them. If we set the window size to be `1`, it means that our model will be accepting `3` words in every pass. We cannot have our model expect `2` words from time to time. The solution is to introduce a special token, such as `<pad>`, that will be added to our sentences to make sure that every word has a valid window around them.\n","\n","\n","\n","> `<pad>` : word window의 size를 맞추기 위해 필요한 `token`.\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"Hx3DrSkLQz4w"}},{"cell_type":"code","source":["# Function that pads the given sentence\n","def pad_window(sentence, window_size, pad_token=\"<pad>\"):\n","  window = [pad_token] * window_size\n","  return window + sentence + window\n","\n","# Show padding example\n","window_size = 2\n","pad_window(train_sentences[0], window_size=window_size)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HKv0bLU-QzkH","executionInfo":{"status":"ok","timestamp":1709373493592,"user_tz":-540,"elapsed":19,"user":{"displayName":"서나미","userId":"11215307119811085502"}},"outputId":"2a612bdc-caeb-42c1-ec3e-e310f5e0de37"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['<pad>', '<pad>', 'we', 'always', 'come', 'to', 'paris', '<pad>', '<pad>']"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","source":["#### 2. Assign an index to each."],"metadata":{"id":"Q4a_vrmjVAWO"}},{"cell_type":"code","source":["# We are just converting our vocabularly to a list to be able to index into it\n","# Sorting is not necessary, we sort to show an ordered word_to_ind dictionary\n","# That being said, we will see that having the index for the padding token\n","# be 0 is convenient as some PyTorch functions use it as a default value\n","# such as nn.utils.rnn.pad_sequence, which we will cover in a bit\n","ix_to_word = sorted(list(vocabulary))\n","\n","\n","# Creating a dictionary to find the index of a given word\n","word_to_ix = {word: ind for ind, word in enumerate(ix_to_word)}"],"metadata":{"id":"KPdflB4gDDnm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Given a sentence of tokens, return the corresponding indices\n","def convert_token_to_indices(sentence, word_to_ix):\n","  indices = []\n","  for token in sentence:\n","    # Check if the token is in our vocabularly. If it is, get it's index.\n","    # If not, get the index for the unknown token.\n","    if token in word_to_ix:\n","      index = word_to_ix[token]\n","    else:\n","      index = word_to_ix[\"<unk>\"]\n","    indices.append(index)\n","  return indices\n","\n","# More compact version of the same function\n","def _convert_token_to_indices(sentence, word_to_ix):\n","  return [word_to_ix.get(token, word_to_ix[\"<unk>\"]) for token in sentence]\n","\n","# Show an example\n","example_sentence = [\"we\", \"always\", \"come\", \"to\", \"kuwait\"]\n","example_indices = _convert_token_to_indices(example_sentence, word_to_ix)\n","restored_example = [ix_to_word[ind] for ind in example_indices]\n","\n","print(f\"Original sentence is: {example_sentence}\")\n","print(f\"Going from words to indices: {example_indices}\")\n","print(f\"Going from indices to words: {restored_example}\")"],"metadata":{"id":"lwZS19WdDTfG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709373493592,"user_tz":-540,"elapsed":15,"user":{"displayName":"서나미","userId":"11215307119811085502"}},"outputId":"6ec5fd45-6d33-4c5b-eb6f-5727ee6a88be"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Original sentence is: ['we', 'always', 'come', 'to', 'kuwait']\n","Going from words to indices: [22, 2, 6, 20, 1]\n","Going from indices to words: ['we', 'always', 'come', 'to', '<unk>']\n"]}]},{"cell_type":"code","source":["# Converting our sentences to indices\n","example_padded_indices = [convert_token_to_indices(s, word_to_ix) for s in train_sentences]\n","example_padded_indices"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MRRzVL-uDYDV","executionInfo":{"status":"ok","timestamp":1709373493592,"user_tz":-540,"elapsed":13,"user":{"displayName":"서나미","userId":"11215307119811085502"}},"outputId":"9729f456-0cfb-4d5a-f6f6-eb08008a31dc"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[22, 2, 6, 20, 15],\n"," [19, 16, 12, 8, 4],\n"," [10, 13, 11, 17],\n"," [9, 7, 8, 18],\n"," [19, 5, 14, 21, 12, 3]]"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","source":["Now that we have an index for each word in our vocabularly, we can create an embedding table with `nn.Embedding` class in `PyTorch`. It is called as follows `nn.Embedding(num_words, embedding_dimension)` where `num_words` is the number of words in our vocabulary and the `embedding_dimension` is the dimension of the embeddings we want to have.\n","\n","`nn.Embedding`: it is just a wrapper class around a trainabe `NxE` dimensional tensor. This table is initially random, but it will change over time. As we train our network, the gradients will be backpropagated all the way to the embedding layer, and hence our word embeddings would be updated."],"metadata":{"id":"H14iPohwYTSc"}},{"cell_type":"code","source":["# Creating an embedding table for our words\n","embedding_dim = 5\n","embeds = nn.Embedding(len(vocabulary), embedding_dim)\n","\n","# Printing the parameters in our embedding table\n","list(embeds.parameters())"],"metadata":{"id":"FnmIFxCtDkPq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709373493593,"user_tz":-540,"elapsed":12,"user":{"displayName":"서나미","userId":"11215307119811085502"}},"outputId":"8e49121a-8e69-436f-ea3e-9f89ced36212"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Parameter containing:\n"," tensor([[ 8.8149e-01,  4.7531e-01, -6.4871e-02, -2.5629e-01,  1.0229e+00],\n","         [-1.8469e-01,  2.3935e+00, -3.3968e-01,  2.1034e-01,  3.9630e-01],\n","         [ 2.2980e+00, -7.5574e-01, -1.3247e+00, -6.4123e-01,  1.7085e-01],\n","         [ 9.2403e-01, -4.8170e-01, -8.6136e-01,  1.1650e+00, -4.1573e-01],\n","         [-1.3079e+00, -7.8322e-02,  5.3715e-01,  5.7253e-01, -2.6159e-01],\n","         [-5.6512e-02, -6.5504e-01, -1.7847e-01, -3.5936e-01,  1.7070e-01],\n","         [ 1.4171e-01,  1.6292e+00, -6.1719e-02, -5.5260e-01,  1.3044e+00],\n","         [-1.7915e-01, -8.5556e-02,  1.0606e+00,  8.7944e-01, -2.7306e-01],\n","         [ 1.8869e-01, -2.0557e+00,  1.0727e+00,  1.1300e+00,  8.1189e-01],\n","         [ 4.9939e-01,  2.8695e-01,  3.6341e-01,  6.6815e-01,  1.3884e-02],\n","         [-1.1570e+00, -5.5916e-01, -1.1770e-01,  7.9884e-01,  2.5116e-01],\n","         [-4.3307e-01,  9.3646e-01, -1.2341e+00,  1.7860e-01,  1.0962e+00],\n","         [-1.6166e+00,  1.4745e-01,  3.2365e-01, -9.4779e-02, -2.0775e+00],\n","         [-4.1048e-01,  1.0507e+00,  1.7883e+00, -1.1094e+00, -2.1309e-01],\n","         [ 1.7908e+00,  3.9137e-01, -1.2953e+00, -6.4793e-01,  7.8394e-01],\n","         [ 6.0334e-01,  2.6560e-01,  5.9233e-02,  1.0394e+00, -1.3990e+00],\n","         [ 1.0238e+00,  5.0794e-01,  3.3043e-01,  8.5140e-01,  1.7933e-01],\n","         [ 1.1571e-01,  3.0552e-01,  1.0540e+00, -1.3579e+00, -3.7791e-01],\n","         [-1.8277e-01,  1.8733e+00, -3.9150e-01, -1.0454e+00, -2.5075e-01],\n","         [-1.7480e+00, -7.7153e-04, -1.9209e-01,  6.3349e-01,  1.0096e-01],\n","         [ 8.0317e-01, -1.7138e+00,  1.5877e+00,  1.7969e-02,  1.0189e-02],\n","         [ 5.9259e-01, -5.0219e-01, -7.8244e-01, -1.4080e+00, -1.5908e+00],\n","         [ 2.9341e-01,  1.8721e-01,  3.8283e-01,  3.0031e-01,  1.4002e-01]],\n","        requires_grad=True)]"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["# We can also get multiple embeddings at once\n","index_paris = word_to_ix[\"paris\"]\n","index_ankara = word_to_ix[\"ankara\"]\n","indices = [index_paris, index_ankara]\n","indices_tensor = torch.tensor(indices, dtype=torch.long)\n","embeddings = embeds(indices_tensor)\n","embeddings"],"metadata":{"id":"T2524j_MD11m","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709373493593,"user_tz":-540,"elapsed":10,"user":{"displayName":"서나미","userId":"11215307119811085502"}},"outputId":"c53d1012-0dfa-46df-b2c5-7234d0fcb292"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 0.6033,  0.2656,  0.0592,  1.0394, -1.3990],\n","        [ 0.9240, -0.4817, -0.8614,  1.1650, -0.4157]],\n","       grad_fn=<EmbeddingBackward0>)"]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","source":["#### Batching Sentences\n","\n","`DataLoader(data, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)`.  The `batch_size` parameter determines the number of examples per batch. In every epoch, we will be iterating over all the batches using the `DataLoader`. The order of batches is deterministic by default, but we can ask `DataLoader` to shuffle the batches by setting the `shuffle` parameter to `True`. This way we ensure that we don't encounter a bad batch multiple times.\n","\n","If provided, `DataLoader` passes the batches it prepares to the `collate_fn`. We can write a custom function to pass to the `collate_fn` parameter in order to print stats about our batch or perform extra processing. In our case, we will use the `collate_fn` to:\n","1. Window pad our train sentences.\n","2. Convert the words in the training examples to indices.\n","3. Pad the training examples so that all the sentences and labels have the same length. Similarly, we also need to pad the labels. This creates an issue because when calculating the loss, we need to know the actual number of words in a given example. We will also keep track of this number in the function we pass to the `collate_fn` parameter.\n","\n","Because our version of the `collate_fn` function will need to access to our `word_to_ix` dictionary (so that it can turn words into indices), we will make use of the `partial` function in `Python`, which passes the parameters we give to the function we pass it."],"metadata":{"id":"QFy8naVkLPyj"}},{"cell_type":"code","source":["from torch.utils.data import DataLoader\n","from functools import partial\n","\n","def custom_collate_fn(batch, window_size, word_to_ix):\n","  # Break our batch into the training examples (x) and labels (y)\n","  # We are turning our x and y into tensors because nn.utils.rnn.pad_sequence\n","  # method expects tensors. This is also useful since our model will be\n","  # expecting tensor inputs.\n","  x, y = zip(*batch)\n","\n","  # Now we need to window pad our training examples. We have already defined a\n","  # function to handle window padding. We are including it here again so that\n","  # everything is in one place.\n","  def pad_window(sentence, window_size, pad_token=\"<pad>\"):\n","    window = [pad_token] * window_size\n","    return window + sentence + window\n","\n","  # Pad the train examples.\n","  x = [pad_window(s, window_size=window_size) for s in x]\n","\n","  # Now we need to turn words in our training examples to indices. We are\n","  # copying the function defined earlier for the same reason as above.\n","  def convert_tokens_to_indices(sentence, word_to_ix):\n","    return [word_to_ix.get(token, word_to_ix[\"<unk>\"]) for token in sentence]\n","\n","  # Convert the train examples into indices.\n","  x = [convert_tokens_to_indices(s, word_to_ix) for s in x]\n","\n","  # We will now pad the examples so that the lengths of all the example in\n","  # one batch are the same, making it possible to do matrix operations.\n","  # We set the batch_first parameter to True so that the returned matrix has\n","  # the batch as the first dimension.\n","  pad_token_ix = word_to_ix[\"<pad>\"]\n","\n","  # pad_sequence function expects the input to be a tensor, so we turn x into one\n","  x = [torch.LongTensor(x_i) for x_i in x]\n","\n","  # nn.utils.rnn.pad_sequence(sequences, batch_first=False, padding_value=0.0)\n","  # Pad a list of variable length Tensors with padding_value.\n","  # sequences (list[Tensor]) – list of variable length sequences.\n","  # batch_first (bool, optional) – output will be in B x T x * if True, or in T x B x * otherwise. Default: False.\n","  # padding_value (float, optional) – value for padded elements. Default: 0.\n","  x_padded = nn.utils.rnn.pad_sequence(x, batch_first=True, padding_value=pad_token_ix)\n","\n","  # We will also pad the labels. Before doing so, we will record the number\n","  # of labels so that we know how many words existed in each example.\n","  lengths = [len(label) for label in y]\n","  lenghts = torch.LongTensor(lengths)\n","\n","  y = [torch.LongTensor(y_i) for y_i in y]\n","  y_padded = nn.utils.rnn.pad_sequence(y, batch_first=True, padding_value=0)\n","\n","  # We are now ready to return our variables. The order we return our variables\n","  # here will match the order we read them in our training loop.\n","  return x_padded, y_padded, lenghts"],"metadata":{"id":"1pIr_HM7LUU8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Set of locations that appear in our corpus\n","locations = set([\"australia\", \"ankara\", \"paris\", \"stanford\", \"taiwan\", \"turkey\"])\n","\n","# Our train labels\n","train_labels = [[1 if word in locations else 0 for word in sent] for sent in train_sentences]"],"metadata":{"id":"WQV1r3KyLqYS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Parameters to be passed to the DataLoader\n","data = list(zip(train_sentences, train_labels))\n","batch_size = 2\n","shuffle = True\n","window_size = 2\n","collate_fn = partial(custom_collate_fn, window_size=window_size, word_to_ix=word_to_ix)\n","\n","# Instantiate the DataLoader\n","loader = DataLoader(data, batch_size=batch_size, shuffle=shuffle, collate_fn=collate_fn)\n","\n","# Go through one loop\n","counter = 0\n","for batched_x, batched_y, batched_lengths in loader:\n","  print(f\"Iteration {counter}\")\n","  print(\"Batched Input:\")\n","  print(batched_x)\n","  print(\"Batched Labels:\")\n","  print(batched_y)\n","  print(\"Batched Lengths:\")\n","  print(batched_lengths)\n","  print(\"\")\n","  counter += 1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6V4ffcFyLcbm","executionInfo":{"status":"ok","timestamp":1709373493593,"user_tz":-540,"elapsed":7,"user":{"displayName":"서나미","userId":"11215307119811085502"}},"outputId":"6dbc03d3-b36d-471d-8dac-6bfa73edfc39"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Iteration 0\n","Batched Input:\n","tensor([[ 0,  0, 19,  5, 14, 21, 12,  3,  0,  0],\n","        [ 0,  0, 19, 16, 12,  8,  4,  0,  0,  0]])\n","Batched Labels:\n","tensor([[0, 0, 0, 1, 0, 1],\n","        [0, 0, 0, 0, 1, 0]])\n","Batched Lengths:\n","tensor([6, 5])\n","\n","Iteration 1\n","Batched Input:\n","tensor([[ 0,  0, 10, 13, 11, 17,  0,  0,  0],\n","        [ 0,  0, 22,  2,  6, 20, 15,  0,  0]])\n","Batched Labels:\n","tensor([[0, 0, 0, 1, 0],\n","        [0, 0, 0, 0, 1]])\n","Batched Lengths:\n","tensor([4, 5])\n","\n","Iteration 2\n","Batched Input:\n","tensor([[ 0,  0,  9,  7,  8, 18,  0,  0]])\n","Batched Labels:\n","tensor([[0, 0, 0, 1]])\n","Batched Lengths:\n","tensor([4])\n","\n"]}]},{"cell_type":"markdown","source":["Our model will be a window classifier. The way our input tensors are currently formatted, we have all the words in a sentence in one datapoint. When we pass this input to our model, it needs to create the windows for each word, make a prediction as to whether the center word is a `LOCATION` or not for each window, put the predictions together and return. So, we formatted our data by breaking it into windows beforehand.\n","\n","Given that our `window_size` is `N` we want our model to make a prediction on every `2N+1` tokens. That is, if we have an input with `9` tokens, and a `window_size` of `2`, we want our model to return `5` predictions. This makes sense because before we padded it with `2` tokens on each side, our input also had `5` tokens in it!\n","\n","We can create these windows by using for loops, but there is a faster `PyTorch` alternative, which is the `unfold(dimension, size, step)` method. We can create the windows we need using this method as follows:"],"metadata":{"id":"x7qWSSWjN84f"}},{"cell_type":"code","source":["# Print the original tensor\n","print(f\"Original Tensor: \")\n","print(batched_x)\n","print(\"\")\n","\n","# Create the 2 * 2 + 1 chunks\n","# Tensor.unfold(dimension, size, step)\n","# dimension (int) – dimension in which unfolding happens\n","# size (int) – the size of each slice that is unfolded\n","# step (int) – the step between each slice\n","chunk = batched_x.unfold(1, window_size*2 + 1, 1)\n","print(f\"Windows: \")\n","print(chunk)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q-6fOXWoLed-","executionInfo":{"status":"ok","timestamp":1709373493593,"user_tz":-540,"elapsed":6,"user":{"displayName":"서나미","userId":"11215307119811085502"}},"outputId":"6a211ea9-d09b-408b-a366-549ec40af23f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Original Tensor: \n","tensor([[ 0,  0,  9,  7,  8, 18,  0,  0]])\n","\n","Windows: \n","tensor([[[ 0,  0,  9,  7,  8],\n","         [ 0,  9,  7,  8, 18],\n","         [ 9,  7,  8, 18,  0],\n","         [ 7,  8, 18,  0,  0]]])\n"]}]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}