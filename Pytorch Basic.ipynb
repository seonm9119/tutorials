{"cells":[{"cell_type":"markdown","source":["# PyTorch\n","> referenced by CS224N, ECCS498\n","\n","[PyTorch](https://pytorch.org/) is an open source machine learning framework. At its core, PyTorch provides a few key features:\n","\n","- A multidimensional **Tensor** object, similar to [numpy](https://numpy.org/) but with GPU accelleration.\n","- An optimized **autograd** engine for automatically computing derivatives\n","- A clean, modular API for building and deploying **deep learning models**\n","\n","You can find more information about PyTorch by following one of the [oficial tutorials](https://pytorch.org/tutorials/) or by [reading the documentation](https://pytorch.org/docs/stable/).\n","\n"],"metadata":{"id":"UJrkbcEJ6dTT"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"xkhmebtIBdIl","executionInfo":{"status":"ok","timestamp":1710487806871,"user_tz":-540,"elapsed":3589,"user":{"displayName":"서나미","userId":"11215307119811085502"}}},"outputs":[],"source":["import torch"]},{"cell_type":"markdown","source":["## Tensor\n","\n","A `torch` **tensor** is\n","- a multidimensional grid of values\n","- all of the same type\n","- indexed by a tuple of nonnegative integers.\n","\n","The number of dimensions is the **rank** of the\n","tensor; the **shape** of a tensor is a tuple of integers giving the size of the array along each dimension. Accessing an element from a PyTorch tensor returns a PyTorch scalar; we can convert this to a Python scalar using the `.item()` method:"],"metadata":{"id":"EuSoejpC3bCV"}},{"cell_type":"markdown","source":["### Tensor constructors & Datatypes"],"metadata":{"id":"falhI4KjK2nm"}},{"cell_type":"code","source":["# From a Python List\n","python_list = [[0, 1], [2, 3],[4, 5]]\n","\n","# From a NumPy Array\n","import numpy as np\n","ndarray = np.array([[4, 5, 6], [7, 8, 9]])\n","\n","print(f\"Python List to Tensor : \\n{torch.Tensor(python_list)}\")\n","print(f\"\\nNumpy Array to Tensor : \\n{torch.from_numpy(ndarray)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WQf456fCmueu","executionInfo":{"status":"ok","timestamp":1710487806872,"user_tz":-540,"elapsed":31,"user":{"displayName":"서나미","userId":"11215307119811085502"}},"outputId":"76aa0406-c499-4040-8213-5d47c064dd21"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Python List to Tensor : \n","tensor([[0., 1.],\n","        [2., 3.],\n","        [4., 5.]])\n","\n","Numpy Array to Tensor : \n","tensor([[4, 5, 6],\n","        [7, 8, 9]])\n"]}]},{"cell_type":"code","source":["# Same as 'torch.zeros(3,2)'\n","shape = (3,2)\n","\n","# Creates a tensor of all zeros\n","zeros_tensor = torch.zeros(shape)\n","\n","# Creates a tensor of all ones\n","ones_tensor = torch.ones(shape)\n","\n","# Creates a 3x3 identity matrix\n","eye_tensor = torch.eye(3)\n","\n","# Creates a tensor of random values\n","# sampled from a uniform distribution between 0 and 1\n","rand_tensor = torch.rand(shape)\n","# sampled from a normal distribution\n","randn_tensor = torch.randn(shape)\n","\n","# Create a tensor with values 0-9\n","x = torch.arange(10)\n","\n","# Create a tensor with value 3.14\n","x = torch.full(shape, 3.14)"],"metadata":{"id":"g_HHjxzypF-i","executionInfo":{"status":"ok","timestamp":1710487806872,"user_tz":-540,"elapsed":21,"user":{"displayName":"서나미","userId":"11215307119811085502"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["data = [[0, 1], [2, 3],[4, 5]]\n","\n","# 'torch.Tensor' defaults to float32\n","tensor1 = torch.Tensor(data)\n","tensor2 = torch.tensor(data, dtype=torch.float32)\n","tensor3 = torch.tensor(data, dtype=torch.float)\n","tensor4 = torch.FloatTensor(data)\n","\n","# Cast a tensor to another datatype using the '.to()' method\n","# '.float()' and '.long()' : cast to particular datatypes\n","tensor5 = tensor1.to(torch.float64)\n","tensor6 = tensor1.long()\n","\n","print(f\"tensor1.dtype : {tensor1.dtype}\")\n","print(f\"tensor2.dtype : {tensor2.dtype}\")\n","print(f\"tensor3.dtype : {tensor3.dtype}\")\n","print(f\"tensor4.dtype : {tensor4.dtype}\")\n","print(f\"tensor5.dtype : {tensor5.dtype}\")\n","print(f\"tensor6.dtype : {tensor6.dtype}\")"],"metadata":{"id":"bbJde5dvLIao","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710487806873,"user_tz":-540,"elapsed":21,"user":{"displayName":"서나미","userId":"11215307119811085502"}},"outputId":"0bcae5e1-211f-47b8-d8bd-1e5edc731ac4"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor1.dtype : torch.float32\n","tensor2.dtype : torch.float32\n","tensor3.dtype : torch.float32\n","tensor4.dtype : torch.float32\n","tensor5.dtype : torch.float64\n","tensor6.dtype : torch.int64\n"]}]},{"cell_type":"markdown","source":["PyTorch provides several ways to create a tensor with the same datatype as another tensor:\n","\n","- `torch.zeros_like()` : Create new tensors with the same shape and type as a given tensor\n","- `.new_zeros()` : Create tensors the same type but possibly different shapes"],"metadata":{"id":"cdb6sBIsrhHs"}},{"cell_type":"code","source":["x = torch.tensor([[0, 1], [2, 3],[4, 5]], dtype=torch.float64)\n","\n","# Create new tensors with the same shape and type as a given tensor\n","zeros_like = torch.zeros_like(x)\n","ones_like = torch.ones_like(x)\n","rand_like = torch.rand_like(x)\n","randn_like = torch.randn_like(x)\n","\n","# Create tensors the same type but possibly different shapes\n","new_zeros = x.new_zeros(4, 5)\n","new_ones = x.new_ones(4,5)"],"metadata":{"id":"yMWuvyCONtO5","executionInfo":{"status":"ok","timestamp":1710487806873,"user_tz":-540,"elapsed":18,"user":{"displayName":"서나미","userId":"11215307119811085502"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["### Tensor Indexing\n","PyTorch provides many other ways of indexing into tensors. Getting comfortable with these different options makes it easy to modify different parts of tensors with ease."],"metadata":{"id":"U7M-dgU1TfnR"}},{"cell_type":"markdown","source":["#### Slice indexing\n","\n","PyTorch tensors can be **sliced** using the syntax `start:stop` or `start:stop:step`. The `stop` index is always non-inclusive:\n","Start and stop indices can be negative, in which case they count backward from the end of the tensor."],"metadata":{"id":"_FHOnkkDFQw6"}},{"cell_type":"code","source":["a = torch.tensor([0, 11, 22, 33, 44, 55, 66])\n","print(0, a)        # (0) Original tensor\n","print(1, a[2:5])   # (1) Elements between index 2 and 5\n","print(2, a[2:])    # (2) Elements after index 2\n","print(3, a[:5])    # (3) Elements before index 5\n","print(4, a[:])     # (4) All elements\n","print(5, a[1:5:2]) # (5) Every second element between indices 1 and 5\n","print(6, a[:-1])   # (6) All but the last element\n","print(7, a[-1:])   # (7) list of the last element\n","print(7, a[-4::2]) # (8) Every second element, starting from the fourth-last"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xzGl362QFZX1","executionInfo":{"status":"ok","timestamp":1710487806874,"user_tz":-540,"elapsed":19,"user":{"displayName":"서나미","userId":"11215307119811085502"}},"outputId":"5a88e0b1-a1fe-428c-9fbe-bee918179d19"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["0 tensor([ 0, 11, 22, 33, 44, 55, 66])\n","1 tensor([22, 33, 44])\n","2 tensor([22, 33, 44, 55, 66])\n","3 tensor([ 0, 11, 22, 33, 44])\n","4 tensor([ 0, 11, 22, 33, 44, 55, 66])\n","5 tensor([11, 33])\n","6 tensor([ 0, 11, 22, 33, 44, 55])\n","7 tensor([66])\n","7 tensor([33, 55])\n"]}]},{"cell_type":"code","source":["mat = torch.tensor([[1,2,3,4], [5,6,7,8], [9,10,11,12]])\n","print(f'Original tensor: \\n{mat}')\n","\n","# Get single row\n","# mat[1, :] same as mat[1]\n","print(f'\\nnmat[1, :] => {mat[1, :]} : {mat[1, :].shape}')\n","print(f'mat[1:2, :] => {mat[1:2, :]} : {mat[1:2, :].shape}')\n","\n","# Get single column\n","# Same as mat[:,1]\n","print(f'\\nmat[:, 1] => {mat[:, 1]} : {mat[:, 1].shape}')\n","print(f'mat[:, 1:2] => \\n{mat[:, 1:2]} : {mat[:, 1:2].shape}')\n","\n","# Get the first two rows and the last three columns\n","print(f'\\nmat[:2, -3:] : \\n{mat[:2, -3:]}')\n","\n","# Get every other row, and columns at index 1 and 2\n","print(f'\\nmat[::2, 1:3] : \\n{mat[::2, 1:3]}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nO8khsAgHwQI","executionInfo":{"status":"ok","timestamp":1710487806874,"user_tz":-540,"elapsed":15,"user":{"displayName":"서나미","userId":"11215307119811085502"}},"outputId":"43e1e054-e20d-425f-a462-9634de2592bf"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Original tensor: \n","tensor([[ 1,  2,  3,  4],\n","        [ 5,  6,  7,  8],\n","        [ 9, 10, 11, 12]])\n","\n","nmat[1, :] => tensor([5, 6, 7, 8]) : torch.Size([4])\n","mat[1:2, :] => tensor([[5, 6, 7, 8]]) : torch.Size([1, 4])\n","\n","mat[:, 1] => tensor([ 2,  6, 10]) : torch.Size([3])\n","mat[:, 1:2] => \n","tensor([[ 2],\n","        [ 6],\n","        [10]]) : torch.Size([3, 1])\n","\n","mat[:2, -3:] : \n","tensor([[2, 3, 4],\n","        [6, 7, 8]])\n","\n","mat[::2, 1:3] : \n","tensor([[ 2,  3],\n","        [10, 11]])\n"]}]},{"cell_type":"markdown","source":["Slicing a tensor returns a **view** into the same data, so modifying it will also modify the original tensor. To avoid this, you can use the `clone()` method to make a copy of a tensor."],"metadata":{"id":"9mtkn1jENOTF"}},{"cell_type":"code","source":["a = torch.tensor([[1, 2], [3, 4], [5, 6]])\n","b = a[:, 1]\n","c = a[:, 1].clone()\n","print(f\"a : \\n{a}\")\n","print(f\"b : {b}\")\n","print(f\"c : {c}\")\n","\n","a[0, 1] = 20  # a[0, 1] and b[0] point to the same element\n","b[1] = 30     # b[1] and a[1, 1] point to the same element\n","c[2] = 40     # c is a clone, so it has its own data\n","print('\\nAfter mutating:')\n","print(f\"a : \\n{a}\")\n","print(f\"b : {b}\")\n","print(f\"c : {c}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v8ra1gsWNRVQ","executionInfo":{"status":"ok","timestamp":1710487806874,"user_tz":-540,"elapsed":11,"user":{"displayName":"서나미","userId":"11215307119811085502"}},"outputId":"845bce65-1594-4243-b0d4-229f3c14078d"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["a : \n","tensor([[1, 2],\n","        [3, 4],\n","        [5, 6]])\n","b : tensor([2, 4, 6])\n","c : tensor([2, 4, 6])\n","\n","After mutating:\n","a : \n","tensor([[ 1, 20],\n","        [ 3, 30],\n","        [ 5,  6]])\n","b : tensor([20, 30,  6])\n","c : tensor([ 2,  4, 40])\n"]}]},{"cell_type":"markdown","source":["we can also use slicing to **modify** subtensors by writing assignment expressions where the left-hand side is a slice expression, and the right-hand side is a constant or a tensor of the correct shape:"],"metadata":{"id":"9OkB7SlHaLLb"}},{"cell_type":"code","source":["a = torch.zeros(2, 4, dtype=torch.int64)\n","a[:, :2] = 1\n","a[:, 2:] = torch.tensor([[2, 3], [4, 5]])\n","print(a)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OEY-UZB3aOlH","executionInfo":{"status":"ok","timestamp":1710487807372,"user_tz":-540,"elapsed":73,"user":{"displayName":"서나미","userId":"11215307119811085502"}},"outputId":"94493612-41c0-4bc6-fdf3-23dbb22776eb"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1, 1, 2, 3],\n","        [1, 1, 4, 5]])\n"]}]},{"cell_type":"markdown","source":["#### Integer tensor indexing\n","\n","When you index into torch tensor using slicing, the resulting tensor view will always be a subarray of the original tensor."],"metadata":{"id":"B0MdNBTyiSjP"}},{"cell_type":"code","source":["mat = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])\n","print('Original tensor:')\n","print(mat)\n","\n","# index arrays can be Python lists of integers\n","idx = [0, 0, 2, 1, 1]\n","print(f'\\nidx => {idx}')\n","print(f'mat[idx] : \\n{mat[idx]}')\n","\n","\n","# Index arrays can be int64 torch tensors\n","idx = torch.tensor([3, 2, 1, 0])\n","print(f'\\nidx => {idx}')\n","print(f'mat[:, idx] : \\n{mat[:, idx]}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4lMFVJTxio5P","executionInfo":{"status":"ok","timestamp":1710487807373,"user_tz":-540,"elapsed":71,"user":{"displayName":"서나미","userId":"11215307119811085502"}},"outputId":"f8201255-5b8e-4b98-e6cb-e8a530d1857a"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Original tensor:\n","tensor([[ 1,  2,  3,  4],\n","        [ 5,  6,  7,  8],\n","        [ 9, 10, 11, 12]])\n","\n","idx => [0, 0, 2, 1, 1]\n","mat[idx] : \n","tensor([[ 1,  2,  3,  4],\n","        [ 1,  2,  3,  4],\n","        [ 9, 10, 11, 12],\n","        [ 5,  6,  7,  8],\n","        [ 5,  6,  7,  8]])\n","\n","idx => tensor([3, 2, 1, 0])\n","mat[:, idx] : \n","tensor([[ 4,  3,  2,  1],\n","        [ 8,  7,  6,  5],\n","        [12, 11, 10,  9]])\n"]}]},{"cell_type":"markdown","source":["We can for example use this to get or set the diagonal of a tensor:"],"metadata":{"id":"1cijbTLdkBew"}},{"cell_type":"code","source":["a = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n","print('Original tensor:')\n","print(a)\n","\n","idx = [0, 1, 2]\n","print(f\"\\nidx => {idx}\")\n","print('Get the diagonal:')\n","print(a[idx, idx])\n","\n","# Modify the diagonal\n","a[idx, idx] = torch.tensor([11, 22, 33])\n","print('\\nAfter setting the diagonal:')\n","print(a)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QDVQUtS8kGsp","executionInfo":{"status":"ok","timestamp":1710487807373,"user_tz":-540,"elapsed":68,"user":{"displayName":"서나미","userId":"11215307119811085502"}},"outputId":"98b73551-59c4-4c4c-ffbe-15c939870a77"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Original tensor:\n","tensor([[1, 2, 3],\n","        [4, 5, 6],\n","        [7, 8, 9]])\n","\n","idx => [0, 1, 2]\n","Get the diagonal:\n","tensor([1, 5, 9])\n","\n","After setting the diagonal:\n","tensor([[11,  2,  3],\n","        [ 4, 22,  6],\n","        [ 7,  8, 33]])\n"]}]},{"cell_type":"markdown","source":["#### One-hot Vector\n","\n","A one-hot vector for an integer $n$ is a vector that has a one in its $n$th slot, and zeros in all other slots. One-hot vectors are commonly used to represent categorical variables in machine learning models. Now creates a matrix of **one-hot vectors** from a list of Python integers."],"metadata":{"id":"_Pj3cTvh294D"}},{"cell_type":"code","source":["idx = [1, 4, 3, 2]\n","one_hot = torch.zeros((len(idx), max(idx)+1), dtype=torch.float32)\n","one_hot[range(len(idx)), idx] = 1\n","print(f\"One-hot Vector : \\n{one_hot}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6IJl7i-o3Qsh","executionInfo":{"status":"ok","timestamp":1710487807374,"user_tz":-540,"elapsed":64,"user":{"displayName":"서나미","userId":"11215307119811085502"}},"outputId":"61f17a9f-16b4-421a-8871-767636084f43"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["One-hot Vector : \n","tensor([[0., 1., 0., 0., 0.],\n","        [0., 0., 0., 0., 1.],\n","        [0., 0., 0., 1., 0.],\n","        [0., 0., 1., 0., 0.]])\n"]}]},{"cell_type":"markdown","source":["#### Masking\n","\n","Boolean tensor indexing lets you pick out arbitrary elements of a tensor according to a boolean mask. Frequently this type of indexing is used to select or modify the elements of a tensor that satisfy some condition."],"metadata":{"id":"UmPHjh4h4wgx"}},{"cell_type":"code","source":["mat = torch.tensor([[1,2], [3, 4], [5, 6]])\n","print('Original tensor:')\n","print(mat)\n","\n","# Find the elements of a that are bigger than 3.\n","# The mask has the same shape as mat\n","mask = (mat > 3)\n","print('\\nMask tensor:')\n","print(mask)\n","\n","# We can use the mask to construct a rank-1 tensor containing the elements of mat\n","# that are selected by the mask\n","print('\\nSelecting elements with the mask:')\n","print(mat[mask])\n","\n","# Set all elements <= 3 to zero:\n","mat[mat <= 3] = 0\n","print('\\nAfter modifying with a mask:')\n","print(mat)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vl8AX7Pa4_N1","executionInfo":{"status":"ok","timestamp":1710487807374,"user_tz":-540,"elapsed":60,"user":{"displayName":"서나미","userId":"11215307119811085502"}},"outputId":"3b185741-2fec-4a7f-d122-dfc58730b2e7"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Original tensor:\n","tensor([[1, 2],\n","        [3, 4],\n","        [5, 6]])\n","\n","Mask tensor:\n","tensor([[False, False],\n","        [False,  True],\n","        [ True,  True]])\n","\n","Selecting elements with the mask:\n","tensor([4, 5, 6])\n","\n","After modifying with a mask:\n","tensor([[0, 0],\n","        [0, 4],\n","        [5, 6]])\n"]}]},{"cell_type":"markdown","source":["## Reshaping operations\n","\n","PyTorch provides many ways to manipulate the shapes of tensors. The simplest example is [`.view()`](https://pytorch.org/docs/stable/generated/torch.Tensor.view.html): This returns a new tensor with the same number of elements as its input, but with a different shape.\n","\n","We can also use `torch.reshape()` method for a similar purpose. There is a subtle difference between `reshape()` and `view()`: `view()` requires the data to be stored contiguously in the memory. You can refer to [this](https://stackoverflow.com/questions/49643225/whats-the-difference-between-reshape-and-view-in-pytorch) StackOverflow answer for more information. In simple terms, contiguous means that the way our data is laid out in the memory is the same as the way we would read elements from it. This happens because some methods, such as `transpose()` and `view()`, do not actually change how our data is stored in the memory. They just change the meta information about out tensor, so that when we use it we will see the elements in the order we expect.\n","\n","`reshape()` calls `view()` internally if the data is stored contiguously, if not, it returns a copy. The difference here isn't too important for basic tensors, but if you perform operations that make the underlying storage of the data non-contiguous (such as taking a transpose), you will have issues using `view()`. If you would like to match the way your tensor is stored in the memory to how it is used, you can use the `contiguous()` method.  "],"metadata":{"id":"XL53hnKZ7SjP"}},{"cell_type":"code","source":["x = torch.tensor([[1, 2], [3, 4], [5, 6]])\n","\n","# Change the shape of x to be 2x3\n","x_view = x.view(2, 3)\n","x_reshaped = torch.reshape(x, (2, 3))\n","\n","# Change the shape of x to be 3x*\n","x_view = x_view.view(3, -1)\n","x_reshaped = torch.reshape(x, (3, -1))"],"metadata":{"id":"bRCGl1NeQfxw","executionInfo":{"status":"ok","timestamp":1710487807374,"user_tz":-540,"elapsed":56,"user":{"displayName":"서나미","userId":"11215307119811085502"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["As a convenience, calls to `.view()` may include a single -1 argument; this puts enough elements on that dimension so that the output has the same number of elements as the input. This makes it easy to write some reshape operations in a way that is agnostic to the shape of the tensor:"],"metadata":{"id":"dGYdTFps_kfO"}},{"cell_type":"code","source":["def flatten(x):\n","    return x.view(-1)\n","\n","def make_row_vec(x):\n","    return x.view(1, -1)"],"metadata":{"id":"tH3Z7H597dbt","executionInfo":{"status":"ok","timestamp":1710487807374,"user_tz":-540,"elapsed":55,"user":{"displayName":"서나미","userId":"11215307119811085502"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["x = torch.tensor([[1, 2], [3, 4], [5, 6]])\n","print(f'Original tensor : {x.shape}')\n","print(x)\n","\n","x_flat = flatten(x)\n","print(f'\\nx_flat : {x_flat.shape}')\n","print(x_flat)\n","\n","x_row = make_row_vec(x)\n","print(f'\\nx_row : {x_row.shape}')\n","print(x_row)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3l7emPpr8Pr1","executionInfo":{"status":"ok","timestamp":1710487807374,"user_tz":-540,"elapsed":55,"user":{"displayName":"서나미","userId":"11215307119811085502"}},"outputId":"0a8bd825-259d-44f3-bd6a-7a5d55e46cc5"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Original tensor : torch.Size([3, 2])\n","tensor([[1, 2],\n","        [3, 4],\n","        [5, 6]])\n","\n","x_flat : torch.Size([6])\n","tensor([1, 2, 3, 4, 5, 6])\n","\n","x_row : torch.Size([1, 6])\n","tensor([[1, 2, 3, 4, 5, 6]])\n"]}]},{"cell_type":"markdown","source":["As its name implies, a tensor returned by `.view()` shares the same data as the input, so changes to one will affect the other and vice-versa:"],"metadata":{"id":"IBxtjAtHAiCN"}},{"cell_type":"code","source":["x = torch.tensor([[1, 2], [3, 4], [5, 6]])\n","x[0, 0] = 10   # x[0, 0] and x_flat[0] point to the same data\n","x_flat[1] = 20 # x_flat[1] and x[0, 1] point to the same data\n","\n","print('x after modifying:')\n","print(x)\n","print('\\nx_flat after modifying:')\n","print(x_flat)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oPVN4kldAjEu","executionInfo":{"status":"ok","timestamp":1710487807375,"user_tz":-540,"elapsed":53,"user":{"displayName":"서나미","userId":"11215307119811085502"}},"outputId":"09568191-bf9c-4c01-b66f-29c6b7e1a980"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["x after modifying:\n","tensor([[10,  2],\n","        [ 3,  4],\n","        [ 5,  6]])\n","\n","x_flat after modifying:\n","tensor([ 1, 20,  3,  4,  5,  6])\n"]}]},{"cell_type":"code","source":["# Add a new dimension of size 1 at the 1st dimension\n","x_unsqueezed = x.unsqueeze(1)\n","print(f\"x_unsqueezed.shape : {x_unsqueezed.shape}\")\n","\n","# Squeeze the dimensions of x by getting rid of all the dimensions with 1 element\n","x_squeezed = x_unsqueezed.squeeze()\n","print(f\"x_squeezed.shape : {x_squeezed.shape}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IAa6KPTkR24u","executionInfo":{"status":"ok","timestamp":1710487807375,"user_tz":-540,"elapsed":48,"user":{"displayName":"서나미","userId":"11215307119811085502"}},"outputId":"283309af-66c4-4c97-cb49-af9f720165b9"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["x_unsqueezed.shape : torch.Size([3, 1, 2])\n","x_squeezed.shape : torch.Size([3, 2])\n"]}]},{"cell_type":"markdown","source":["### Transposing a Matrix\n","\n","Another common reshape operation you might want to perform is transposing a matrix. You might be surprised if you try to transpose a matrix with .view(): The view() function takes elements in row-major order, so you cannot transpose matrices with .view()."],"metadata":{"id":"mh-krxErB-oq"}},{"cell_type":"code","source":["x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n","print('Original matrix:')\n","print(x)\n","print('\\nTransposing with view DOES NOT WORK!')\n","print(x.view(3, 2))\n","print('\\nTransposed matrix:')\n","\n","# Same as torch.t(x)\n","print(x.t())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Afl7rhyEA_YC","executionInfo":{"status":"ok","timestamp":1710487807375,"user_tz":-540,"elapsed":44,"user":{"displayName":"서나미","userId":"11215307119811085502"}},"outputId":"65eac1e6-c106-431b-834c-708ffffe6faa"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Original matrix:\n","tensor([[1, 2, 3],\n","        [4, 5, 6]])\n","\n","Transposing with view DOES NOT WORK!\n","tensor([[1, 2],\n","        [3, 4],\n","        [5, 6]])\n","\n","Transposed matrix:\n","tensor([[1, 4],\n","        [2, 5],\n","        [3, 6]])\n"]}]},{"cell_type":"code","source":["# For tensors with more than two dimensions,\n","\n","# Create a tensor of shape (2, 3, 4)\n","x = torch.tensor([\n","     [[1,  2,  3,  4],\n","      [5,  6,  7,  8],\n","      [9, 10, 11, 12]],\n","     [[13, 14, 15, 16],\n","      [17, 18, 19, 20],\n","      [21, 22, 23, 24]]])\n","print(f'Original tensor : {x.shape}')\n","print(x)\n","\n","# Swap axes 1 and 2; shape is (2, 4, 3)\n","x1 = x.transpose(1, 2)\n","print(f'\\nSwap axes 1 and 2 : {x1.shape}')\n","print(x1)\n","\n","# Permute axes; the argument (1, 2, 0) means:\n","# - Make the old dimension 1 appear at dimension 0;\n","# - Make the old dimension 2 appear at dimension 1;\n","# - Make the old dimension 0 appear at dimension 2\n","# This results in a tensor of shape (3, 4, 2)\n","x2 = x.permute(1, 2, 0)\n","print(f'\\nPermute axes : {x2.shape}')\n","print(x2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"izeo9k9PCShX","executionInfo":{"status":"ok","timestamp":1710487807375,"user_tz":-540,"elapsed":40,"user":{"displayName":"서나미","userId":"11215307119811085502"}},"outputId":"d0c464ef-11e0-4f85-c258-acbbaa48a880"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Original tensor : torch.Size([2, 3, 4])\n","tensor([[[ 1,  2,  3,  4],\n","         [ 5,  6,  7,  8],\n","         [ 9, 10, 11, 12]],\n","\n","        [[13, 14, 15, 16],\n","         [17, 18, 19, 20],\n","         [21, 22, 23, 24]]])\n","\n","Swap axes 1 and 2 : torch.Size([2, 4, 3])\n","tensor([[[ 1,  5,  9],\n","         [ 2,  6, 10],\n","         [ 3,  7, 11],\n","         [ 4,  8, 12]],\n","\n","        [[13, 17, 21],\n","         [14, 18, 22],\n","         [15, 19, 23],\n","         [16, 20, 24]]])\n","\n","Permute axes : torch.Size([3, 4, 2])\n","tensor([[[ 1, 13],\n","         [ 2, 14],\n","         [ 3, 15],\n","         [ 4, 16]],\n","\n","        [[ 5, 17],\n","         [ 6, 18],\n","         [ 7, 19],\n","         [ 8, 20]],\n","\n","        [[ 9, 21],\n","         [10, 22],\n","         [11, 23],\n","         [12, 24]]])\n"]}]},{"cell_type":"markdown","source":["### Contiguous tensors\n","\n","Some combinations of reshaping operations will fail with cryptic errors. The exact reasons for this have to do with the way that tensors and views of tensors are implemented, and are beyond the scope of this assignment. However if you're curious, [this blog post by Edward Yang](http://blog.ezyang.com/2019/05/pytorch-internals/) gives a clear explanation of the problem.\n","\n","What you need to know is that you can typically overcome these sorts of errors by either by calling [`.contiguous()`](https://pytorch.org/docs/stable/generated/torch.Tensor.contiguous.html) before `.view()`, or by using [`.reshape()`](https://pytorch.org/docs/stable/generated/torch.reshape.html) instead of `.view()`."],"metadata":{"id":"u9A9zCQIDjCA"}},{"cell_type":"code","source":["x = torch.randn(2, 3, 4)\n","\n","try:\n","  # This sequence of reshape operations will crash\n","  x1 = x.transpose(1, 2).view(8, 3)\n","except RuntimeError as e:\n","  print(type(e), e)\n","\n","# We can solve the problem using either .contiguous() or .reshape()\n","x1 = x.transpose(1, 2).contiguous().view(8, 3)\n","x2 = x.transpose(1, 2).reshape(8, 3)\n","print('x1 shape: ', x1.shape)\n","print('x2 shape: ', x2.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yZMKEZBrDpZC","executionInfo":{"status":"ok","timestamp":1710487807375,"user_tz":-540,"elapsed":37,"user":{"displayName":"서나미","userId":"11215307119811085502"}},"outputId":"6a41e16b-ec25-48ee-e5d6-5c4ed1828a48"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'RuntimeError'> view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.\n","x1 shape:  torch.Size([8, 3])\n","x2 shape:  torch.Size([8, 3])\n"]}]},{"cell_type":"markdown","source":["### Exercise\n","Given the 1-dimensional input tensor `x` containing the numbers 0 through 23 in order, it should the following output tensor `y` of shape `(3, 8)` by using reshape operations on x:\n","\n","\n","```\n","y = tensor([[ 0,  1,  2,  3, 12, 13, 14, 15],\n","            [ 4,  5,  6,  7, 16, 17, 18, 19],\n","            [ 8,  9, 10, 11, 20, 21, 22, 23]])\n","```"],"metadata":{"id":"XOk-mMhNGdDw"}},{"cell_type":"code","source":["x = torch.arange(24)\n","print('Here is x:')\n","print(x)\n","\n","y = x.view(-1, 3, 4)\n","print('\\nHere is y:')\n","print(y)\n","\n","y = y.permute(1, 0, 2)\n","print('\\nHere is y:')\n","print(y)\n","\n","y = y.reshape(3, 8)\n","print('\\nHere is y:')\n","print(y)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nzNwbSWoGTsu","executionInfo":{"status":"ok","timestamp":1710487807375,"user_tz":-540,"elapsed":33,"user":{"displayName":"서나미","userId":"11215307119811085502"}},"outputId":"bc82c356-7172-4794-c3b8-4153e74dfb50"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Here is x:\n","tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n","        18, 19, 20, 21, 22, 23])\n","\n","Here is y:\n","tensor([[[ 0,  1,  2,  3],\n","         [ 4,  5,  6,  7],\n","         [ 8,  9, 10, 11]],\n","\n","        [[12, 13, 14, 15],\n","         [16, 17, 18, 19],\n","         [20, 21, 22, 23]]])\n","\n","Here is y:\n","tensor([[[ 0,  1,  2,  3],\n","         [12, 13, 14, 15]],\n","\n","        [[ 4,  5,  6,  7],\n","         [16, 17, 18, 19]],\n","\n","        [[ 8,  9, 10, 11],\n","         [20, 21, 22, 23]]])\n","\n","Here is y:\n","tensor([[ 0,  1,  2,  3, 12, 13, 14, 15],\n","        [ 4,  5,  6,  7, 16, 17, 18, 19],\n","        [ 8,  9, 10, 11, 20, 21, 22, 23]])\n"]}]},{"cell_type":"markdown","source":["## Tensor operations"],"metadata":{"id":"lXg4X5njaVXZ"}},{"cell_type":"markdown","source":["### Elementwise operations\n","Basic mathematical functions operate elementwise on tensors, and are available as operator overloads, as functions in the `torch` module, and as instance methods on torch objects; all produce the same results:"],"metadata":{"id":"roccEMwsKXmg"}},{"cell_type":"code","source":["x = torch.tensor([[1, 2, 3, 4]], dtype=torch.float32)\n","y = torch.tensor([[5, 6, 7, 8]], dtype=torch.float32)\n","\n","\n","# Same as `torch.add(x, y)` and `x.add(y)`\n","element_sum = x + y\n","\n","# Same as `torch.sub(x, y)` and `x.sub(y)`\n","element_diff = x - y\n","\n","# Same as `torch.mul(x, y)` and `x.mul(y)`\n","element_mul = x * y\n","\n","# Same as `torch.div(x, y)` and `x.div(y)`\n","element_div = x / y\n","\n","# Same as `torch.pow(x, y)` and `x.pow(y)`\n","element_pow = x ** y"],"metadata":{"id":"PbBEW_r7Heb6","executionInfo":{"status":"ok","timestamp":1710487807376,"user_tz":-540,"elapsed":30,"user":{"displayName":"서나미","userId":"11215307119811085502"}}},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":["Torch also provides many standard mathematical functions; these are available both as functions in the `torch` module and as instance methods on tensors:\n","\n","You can find a full list of all available mathematical functions [in the documentation](https://pytorch.org/docs/stable/torch.html#pointwise-ops); many functions in the `torch` module have corresponding instance methods [on tensor objects](https://pytorch.org/docs/stable/tensors.html)."],"metadata":{"id":"jSJdc5JQI7Bf"}},{"cell_type":"code","source":["x = torch.tensor([[1, 2, 3, 4]], dtype=torch.float32)\n","\n","# Same as `torch.sqrt(x)`\n","print(x.sqrt())\n","\n","# Same as `torch.sin(x)`\n","print(x.sin())\n","\n","# Same as `torch.cos(x)`\n","print(x.cos())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ENpudqDvI_Yf","executionInfo":{"status":"ok","timestamp":1710487807376,"user_tz":-540,"elapsed":29,"user":{"displayName":"서나미","userId":"11215307119811085502"}},"outputId":"b6c77994-90ec-4e62-c32b-9483ea0e0ee2"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1.0000, 1.4142, 1.7321, 2.0000]])\n","tensor([[ 0.8415,  0.9093,  0.1411, -0.7568]])\n","tensor([[ 0.5403, -0.4161, -0.9900, -0.6536]])\n"]}]},{"cell_type":"markdown","source":["### Reduction operations\n","\n","We may sometimes want to perform operations that aggregate over part or all of a tensor, such as a summation; these are called reduction operations.\n","\n","Like the elementwise operations above, most reduction operations are available both as functions in the torch module and as instance methods on tensor objects."],"metadata":{"id":"Syu1VE7PKSPy"}},{"cell_type":"code","source":["x = torch.tensor([[6, 1, 5],\n","                  [3, 4, 2]], dtype=torch.float32)\n","print(f'Original tensor: {x.shape}')\n","print(x)\n","\n","# Same as `torch.sum(x)`\n","print('\\nSum over entire tensor:')\n","print(x.sum())\n","\n","# Same as `torch.sum(x, dim=0)`\n","print(f'\\nx.sum(dim=0) : \\n{x.sum(dim=0)}')\n","\n","# Same as `torch.sum(x, dim=1)`\n","print(f'\\nx.sum(dim=1) : \\n{x.sum(dim=1)}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"13HrmQxEKd-x","executionInfo":{"status":"ok","timestamp":1710487807377,"user_tz":-540,"elapsed":26,"user":{"displayName":"서나미","userId":"11215307119811085502"}},"outputId":"696ff3ee-d448-4e91-e208-42d53f0cd4db"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Original tensor: torch.Size([2, 3])\n","tensor([[6., 1., 5.],\n","        [3., 4., 2.]])\n","\n","Sum over entire tensor:\n","tensor(21.)\n","\n","x.sum(dim=0) : \n","tensor([9., 5., 7.])\n","\n","x.sum(dim=1) : \n","tensor([12.,  9.])\n"]}]},{"cell_type":"code","source":["print('Original tensor:')\n","print(x)\n","\n","# Finding the overall minimum only returns a single value\n","print('\\nOverall minimum: ', x.min())\n","\n","# Compute the minimum along each column or row\n","# row_min_vals, row_min_idxs = x.min(dim=1)\n","col_min_vals, col_min_idxs = x.min(dim=0)\n","row_argmin_idxs = x.argmin(dim=1)\n","\n","print('\\nMinimum along each column:')\n","print(f'values: {col_min_vals} \\tidxs: {col_min_idxs}')\n","print(f'argmin_idxs: {row_argmin_idxs}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TK0I_NyYOWbD","executionInfo":{"status":"ok","timestamp":1710487807377,"user_tz":-540,"elapsed":23,"user":{"displayName":"서나미","userId":"11215307119811085502"}},"outputId":"8d70460d-91ef-4ce4-e205-902eacd4d906"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["Original tensor:\n","tensor([[6., 1., 5.],\n","        [3., 4., 2.]])\n","\n","Overall minimum:  tensor(1.)\n","\n","Minimum along each column:\n","values: tensor([3., 1., 2.]) \tidxs: tensor([1, 0, 1])\n","argmin_idxs: tensor([1, 2])\n"]}]},{"cell_type":"code","source":["print('Original tensor:')\n","print(x)\n","\n","print(\"\\nOverall Mean: {}\".format(x.mean()))\n","print(\"Mean in the 0th dimension: {}\".format(x.mean(dim=0)))\n","print(\"Mean in the 1st dimension: {}\".format(x.mean(dim=1)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v7AdklXswDqY","executionInfo":{"status":"ok","timestamp":1710487807377,"user_tz":-540,"elapsed":19,"user":{"displayName":"서나미","userId":"11215307119811085502"}},"outputId":"5c5da26b-6f56-45ea-fbfe-b7405fcdf2a3"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["Original tensor:\n","tensor([[6., 1., 5.],\n","        [3., 4., 2.]])\n","\n","Overall Mean: 3.5\n","Mean in the 0th dimension: tensor([4.5000, 2.5000, 3.5000])\n","Mean in the 1st dimension: tensor([4., 3.])\n"]}]},{"cell_type":"markdown","source":["Some people often get confused by the `dim` argument in reduction operations.\n","\n","The easiest way to remember is to think about the shapes of the tensors involved.\n","After summing with `dim=d`, the dimension at index `d` of the input is **eliminated** from the shape of the output tensor:\n","\n","Reduction operations *reduce* the rank of tensors: the dimension over which you perform the reduction will be removed from the shape of the output. If you pass `keepdim=True` to a reduction operation, the specified dimension will not be removed; the output tensor will instead have a shape of 1 in that dimension."],"metadata":{"id":"pKXVEkh4L1Uc"}},{"cell_type":"code","source":["# Create a tensor of shape (128, 10, 3, 64, 64)\n","x = torch.randn(128, 10, 3, 64, 64)\n","print(x.shape)\n","\n","# Take the mean over dimension 1; shape is now (128, 3, 64, 64)\n","x = x.mean(dim=1)\n","print(x.shape)\n","\n","# Take the sum over dimension 2; shape is now (128, 3, 64)\n","x = x.sum(dim=2)\n","print(x.shape)\n","\n","# Take the mean over dimension 1, but keep the dimension from being eliminated\n","# by passing keepdim=True; shape is now (128, 1, 64)\n","x = x.mean(dim=1, keepdim=True)\n","print(x.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ru24FJOrL5Oh","executionInfo":{"status":"ok","timestamp":1710487808311,"user_tz":-540,"elapsed":950,"user":{"displayName":"서나미","userId":"11215307119811085502"}},"outputId":"38370d2b-da42-494d-fe3f-1d1f63cbb202"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([128, 10, 3, 64, 64])\n","torch.Size([128, 3, 64, 64])\n","torch.Size([128, 3, 64])\n","torch.Size([128, 1, 64])\n"]}]},{"cell_type":"markdown","source":["### Matrix operations\n","\n","PyTorch provides a number of linear algebra functions that compute different types of vector and matrix products. You can find a full list of the available linear algebra operators [in the documentation](https://pytorch.org/docs/stable/torch.html#blas-and-lapack-operations).\n","All of these functions are also available as Tensor instance methods, e.g. [`Tensor.dot`](https://pytorch.org/docs/stable/generated/torch.Tensor.dot.html) instead of `torch.dot`."],"metadata":{"id":"z6s_6c-NUlg1"}},{"cell_type":"code","source":["# Shape of vectors : 1x2\n","vec1 = torch.tensor([9, 10], dtype=torch.float32)\n","vec2 = torch.tensor([11, 12], dtype=torch.float32)\n","\n","mat1 = torch.tensor([[1,2],[3,4]], dtype=torch.float32)\n","mat2 = torch.tensor([[5,6],[7,8]], dtype=torch.float32)\n","\n","# Inner product of vectors\n","# `dot` only works for vector-vector products.\n","# Same as `torch.dot(vec1, vec2)`\n","print(vec1.dot(vec2))\n","\n","# Matrix-matrix products:\n","print('\\nMatrix-Matrix product :' )\n","# Same as `torch.mm(x, y)`\n","print(mat1.mm(mat2))\n","\n","# Matrix-vector multiply\n","print('\\nMatrix-vector product (rank 1 output) :')\n","# Same as `torch.mv(mat1, vec1)`\n","print(mat1.mv(vec1))\n","\n","# We can reshape the vector to have rank 2 and use torch.mm to perform\n","# matrix-vector products, but the result will have rank 2\n","print('\\nMatrix-vector product (rank 2 output) :')\n","# Same as `torch.mm(mat1, vec1.view(2, 1))`\n","print(mat1.mm(vec1.view(2, 1)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oKeOlkQVVN-h","executionInfo":{"status":"ok","timestamp":1710487808312,"user_tz":-540,"elapsed":74,"user":{"displayName":"서나미","userId":"11215307119811085502"}},"outputId":"29e68ab6-81d5-4abb-c2d8-e4200e0b2446"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(219.)\n","\n","Matrix-Matrix product :\n","tensor([[19., 22.],\n","        [43., 50.]])\n","\n","Matrix-vector product (rank 1 output) :\n","tensor([29., 67.])\n","\n","Matrix-vector product (rank 2 output) :\n","tensor([[29.],\n","        [67.]])\n"]}]},{"cell_type":"markdown","source":["#### Batched matrix multiply\n","\n","PyTorch에서는 행렬 곱셈에 대해 torch.bmm과 torch.matmul이라는 두 가지 주요 함수를 제공한다.\n","\n","`torch.bmm`은 배치된 행렬 곱셈을 위해 특별히 설계되었다. 입력 텐서의 모양이 `(B, N, M)`이고 두 번째 텐서의 모양이 `(B, M, P)`인 경우, 각 배치 요소에 대한 행렬 곱을 계산하고 모양이 `(B, N, P)`인 텐서를 반환한다.\n","\n","`torch.matmul`는 더 많은 유연성을 제공하며 여러 차원에서의 행렬 곱셈과 브로드캐스팅을 지원한다. torch.matmul의 동작은 입력 텐서의 차원에 따라 달라지며, 자세한 사항은 [여기](https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul)에서 확인가능하다.\n","\n"],"metadata":{"id":"KM4g--sZv-T_"}},{"cell_type":"code","source":["# Perform batched matrix multiplication between the tensor x and y\n","# x : (B, N, M)\n","# y : (B, M, P)\n","# z : (B, N, P)\n","B, N, M, P = 2, 3, 5, 4\n","x = torch.randn(B, N, M) # 2x3x5\n","y = torch.randn(B, M, P) # 2x5x4\n","\n","# We want a tensor z of shape (2, 3, 4)"],"metadata":{"id":"RmjInii_v9nM","executionInfo":{"status":"ok","timestamp":1710487808312,"user_tz":-540,"elapsed":70,"user":{"displayName":"서나미","userId":"11215307119811085502"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["loop_z = torch.stack([x[i].mm(y[i]) for i in range(B)])\n","bmm_z = x.bmm(y)\n","\n","# Same as `torch.matmul(mat1, vec1)`\n","# Same as `mat1@vec1`\n","matmul_z = x.matmul(y)"],"metadata":{"id":"QSwW5iktwWJ6","executionInfo":{"status":"ok","timestamp":1710487808312,"user_tz":-540,"elapsed":69,"user":{"displayName":"서나미","userId":"11215307119811085502"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["print(f\"loop_z.shape : {loop_z.shape}\")\n","print(f\"bmm_z.shape : {bmm_z.shape}\")\n","print(f\"matmul_z.shape : {matmul_z.shape}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jOprF-dixWA2","executionInfo":{"status":"ok","timestamp":1710487808312,"user_tz":-540,"elapsed":68,"user":{"displayName":"서나미","userId":"11215307119811085502"}},"outputId":"eae329b4-b438-4fd1-a8a0-1d1674d70401"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["loop_z.shape : torch.Size([2, 3, 4])\n","bmm_z.shape : torch.Size([2, 3, 4])\n","matmul_z.shape : torch.Size([2, 3, 4])\n"]}]},{"cell_type":"markdown","source":["## Broadcasting\n","\n","Broadcasting is a powerful mechanism that allows PyTorch to work with arrays of different shapes when performing arithmetic operations.\n","\n","For example, suppose that we want to add a constant vector to each row of a tensor. PyTorch broadcasting allows us to perform this computation without actually creating multiple copies of v. Consider this version, using broadcasting:\n","\n","Ttry reading the explanation from the [documentation](https://pytorch.org/docs/stable/notes/broadcasting.html).\n","\n","Broadcasting usually happens implicitly inside many PyTorch operators. However we can also broadcast explicitly using the function [`torch.broadcast_tensors`](https://pytorch.org/docs/stable/generated/torch.broadcast_tensors.html#torch.broadcast_tensors):"],"metadata":{"id":"D7uVE9ic1i-m"}},{"cell_type":"code","source":["# We will add the vector v to each row of the matrix x,\n","# storing the result in the matrix y\n","x = torch.tensor([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]])\n","v = torch.tensor([1, 0, 1])\n","y = x + v  # Add v to each row of x using broadcasting\n","print(y)\n","\n","xx, vv = torch.broadcast_tensors(x, v)\n","print(f'\\nHere is xx (after) broadcasting): \\n{xx}')\n","print(f'\\nHere is vv (after broadcasting): \\n{vv}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jvGCJ0bg2UFb","executionInfo":{"status":"ok","timestamp":1710487808312,"user_tz":-540,"elapsed":64,"user":{"displayName":"서나미","userId":"11215307119811085502"}},"outputId":"3db339fb-6c75-45b9-df70-0b15bd587e5a"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 2,  2,  4],\n","        [ 5,  5,  7],\n","        [ 8,  8, 10],\n","        [11, 11, 13]])\n","\n","Here is xx (after) broadcasting): \n","tensor([[ 1,  2,  3],\n","        [ 4,  5,  6],\n","        [ 7,  8,  9],\n","        [10, 11, 12]])\n","\n","Here is vv (after broadcasting): \n","tensor([[1, 0, 1],\n","        [1, 0, 1],\n","        [1, 0, 1],\n","        [1, 0, 1]])\n"]}]},{"cell_type":"markdown","source":["Broadcasting can let us easily implement many different operations. For example we can compute an outer product of vectors:"],"metadata":{"id":"tepdhlGQ3f7n"}},{"cell_type":"code","source":["# Compute outer product of vectors\n","v = torch.tensor([1, 2, 3])  # v has shape (3,)\n","w = torch.tensor([4, 5])     # w has shape (2,)\n","# To compute an outer product, we first reshape v to be a column\n","# vector of shape (3, 1); we can then broadcast it against w to yield\n","# an output of shape (3, 2), which is the outer product of v and w:\n","print(v.view(3, 1) * w)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hf7mFoN93ExF","executionInfo":{"status":"ok","timestamp":1710487808312,"user_tz":-540,"elapsed":61,"user":{"displayName":"서나미","userId":"11215307119811085502"}},"outputId":"5276c4c5-b529-4c2c-9fa6-cfef528bc371"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 4,  5],\n","        [ 8, 10],\n","        [12, 15]])\n"]}]},{"cell_type":"markdown","source":["## GPU\n","\n","One of the most important features of PyTorch is that it can use graphics processing units (GPUs) to accelerate its tensor operations.\n","\n","We can easily check whether PyTorch is configured to use GPUs:\n","\n","Tensors can be moved onto any device using the .to method."],"metadata":{"id":"ZIxv7ULyKs9P"}},{"cell_type":"code","source":["import torch\n","\n","if torch.cuda.is_available():\n","  print('PyTorch can use GPUs!')\n","else:\n","  print('PyTorch cannot use GPUs.')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aR6F4wfSK2vP","executionInfo":{"status":"ok","timestamp":1710487808312,"user_tz":-540,"elapsed":58,"user":{"displayName":"서나미","userId":"11215307119811085502"}},"outputId":"edc6af7b-79b2-4720-8796-5d80d30c57b3"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["PyTorch can use GPUs!\n"]}]},{"cell_type":"markdown","source":["All PyTorch tensors also have a `device` attribute that specifies the device where the tensor is stored -- either CPU, or CUDA (for NVIDA GPUs). A tensor on a CUDA device will automatically use that device to accelerate all of its operations.\n","\n","Just as with datatypes, we can use the [`.to()`](https://pytorch.org/docs/1.1.0/tensors.html#torch.Tensor.to) method to change the device of a tensor. We can also use the convenience methods `.cuda()` and `.cpu()` methods to move tensors between CPU and GPU."],"metadata":{"id":"4D_JdIqKLAtQ"}},{"cell_type":"code","source":["# Construct a tensor on the CPU\n","cpu_x = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32)\n","\n","gpu_x = torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.float64, device='cuda')\n","\n","# Move it to the GPU\n","# Same as `cpu_x.to('cuda')`\n","x1 = cpu_x.cuda()\n","print('x1 device:', x1.device)\n","\n","# Move it to the CPU\n","# Same as `gpu_x.to('cpu')`\n","x2 = gpu_x.cpu()\n","print('x2 device:', x2.device)\n","\n","\n","# Calling cpu_x.to(gpu_x) where gpu_x is a tensor will return a copy of cpu_x with the same\n","# device and dtype as gpu_x\n","x3 = cpu_x.to(gpu_x)\n","print('x3 device / dtype:', x3.device, x3.dtype)"],"metadata":{"id":"vK8FEy22MQdD","executionInfo":{"status":"ok","timestamp":1710487808312,"user_tz":-540,"elapsed":56,"user":{"displayName":"서나미","userId":"11215307119811085502"}},"outputId":"e6752ee3-f796-4b12-dbff-b170b4248e38","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["x1 device: cuda:0\n","x2 device: cpu\n","x3 device / dtype: cuda:0 torch.float64\n"]}]},{"cell_type":"markdown","source":["## Tensor Properties"],"metadata":{"id":"Ar0sb3D0QlK1"}},{"cell_type":"code","source":["x = torch.tensor([[1,2,3], [4,5,6]])\n","\n","print(f\"Hear is x : \\n{x}\")\n","\n","# Get the number of elements in tensor.\n","print(f\"\\nNumber of elements : {x.numel()}\")\n","\n","# Transform to list\n","print(f\"to_list : {x.tolist()}\")\n","\n","# Stack 2 copies of v on top of each other\n","xx = x.repeat((2, 1))\n","print(f\"\\nx.repeat : \\n{xx}\")\n","\n","print('\\nx is a tensor:', torch.is_tensor(x))\n","print('x is filled with 0: ', (x == 0).all().item() == 1)\n","print('rank of a: ', x.dim())\n","print('type(a[0].item()): ', x[0,0].item())\n","\n"],"metadata":{"id":"GAxkSXYeSy-I","executionInfo":{"status":"ok","timestamp":1710487808312,"user_tz":-540,"elapsed":51,"user":{"displayName":"서나미","userId":"11215307119811085502"}},"outputId":"d3e8840d-90ec-44ce-a0c5-422a5f1ec9bf","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["Hear is x : \n","tensor([[1, 2, 3],\n","        [4, 5, 6]])\n","\n","Number of elements : 6\n","to_list : [[1, 2, 3], [4, 5, 6]]\n","\n","x.repeat : \n","tensor([[1, 2, 3],\n","        [4, 5, 6],\n","        [1, 2, 3],\n","        [4, 5, 6]])\n","\n","x is a tensor: True\n","x is filled with 0:  False\n","rank of a:  2\n","type(a[0].item()):  1\n"]}]},{"cell_type":"code","source":["# Concatenate in dimension 0 and 1\n","x_cat0 = torch.cat([x, x, x], dim=0)\n","x_cat1 = torch.cat([x, x, x], dim=1)\n","\n","print(\"Initial shape: {}\".format(x.shape))\n","print(\"Shape after concatenation in dimension 0: {}\".format(x_cat0.shape))\n","print(\"Shape after concatenation in dimension 1: {}\".format(x_cat1.shape))"],"metadata":{"id":"Ryx-RACZwmP4","executionInfo":{"status":"ok","timestamp":1710487808312,"user_tz":-540,"elapsed":42,"user":{"displayName":"서나미","userId":"11215307119811085502"}},"outputId":"119afd13-7eba-449d-a399-466e9f3ee3b8","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["Initial shape: torch.Size([2, 3])\n","Shape after concatenation in dimension 0: torch.Size([6, 3])\n","Shape after concatenation in dimension 1: torch.Size([2, 9])\n"]}]},{"cell_type":"markdown","source":["## Autograd\n","\n","We can see that the `x.grad` is updated to be the sum of the gradients calculated so far. When we run backprop in a neural network, we sum up all the gradients for a particular neuron before making an update. This is exactly what is happening here! This is also the reason why we need to run `zero_grad()` in every training iteration (more on this later). Otherwise our gradients would keep building up from one training iteration to the other, which would cause our updates to be wrong."],"metadata":{"id":"1IAhIXp2xenc"}},{"cell_type":"code","source":["# Create an example tensor\n","# requires_grad parameter tells PyTorch to store gradients\n","x = torch.tensor([2.], requires_grad=True)\n","\n","# Print the gradient if it is calculated\n","# Currently None since x is a scalar\n","print(f\"x.grad : {x.grad}\")"],"metadata":{"id":"ZXPZujIJxg3n","executionInfo":{"status":"ok","timestamp":1710487808312,"user_tz":-540,"elapsed":38,"user":{"displayName":"서나미","userId":"11215307119811085502"}},"outputId":"19d6b64e-c436-439d-c348-ea9ce50c4457","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["x.grad : None\n"]}]},{"cell_type":"code","source":["# Calculating the gradient of y with respect to x\n","y = x * x * 3 # 3x^2\n","y.backward()\n","print(f\"x.grad : {x.grad}\")"],"metadata":{"id":"GpsHvvScx1wj","executionInfo":{"status":"ok","timestamp":1710487808313,"user_tz":-540,"elapsed":33,"user":{"displayName":"서나미","userId":"11215307119811085502"}},"outputId":"37cdd0f7-eac7-4911-9804-9340f0b65a6f","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["x.grad : tensor([12.])\n"]}]},{"cell_type":"code","source":["z = x * x * 3 # 3x^2\n","z.backward()\n","print(f\"x.grad : {x.grad}\")"],"metadata":{"id":"UV6RT8QgyULa","executionInfo":{"status":"ok","timestamp":1710487808313,"user_tz":-540,"elapsed":29,"user":{"displayName":"서나미","userId":"11215307119811085502"}},"outputId":"93a91b9e-d834-406d-cc6a-dd7ecb56ecc6","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["x.grad : tensor([24.])\n"]}]},{"cell_type":"markdown","source":["## Neural Network Module"],"metadata":{"id":"s4yYaFjhfaxn"}},{"cell_type":"code","source":["import torch.nn as nn"],"metadata":{"id":"qL36qRcnfiv5","executionInfo":{"status":"ok","timestamp":1710487808313,"user_tz":-540,"elapsed":25,"user":{"displayName":"서나미","userId":"11215307119811085502"}}},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":["### Linear Layer\n","\n","We can use `nn.Linear(H_in, H_out)` to create a a linear layer. This will take a matrix of `(N, *, H_in)` dimensions and output a matrix of `(N, *, H_out)`. The `*` denotes that there could be arbitrary number of dimensions in between. The linear layer performs the operation `Ax+b`, where `A` and `b` are initialized randomly. If we don't want the linear layer to learn the bias parameters, we can initialize our layer with `bias=False`."],"metadata":{"id":"Gkz9_VR5fkOV"}},{"cell_type":"code","source":["# Create the inputs, (N, *, H_in)\n","input = torch.ones(2,3,4)\n","\n","# Make a linear layers transforming (N, *, H_in) dimensinal inputs to (N, *, H_out)\n","linear = nn.Linear(4, 2) # (H_in, H_out)\n","linear_output = linear(input)\n","linear_output"],"metadata":{"id":"Cqd19lSgf6E3","executionInfo":{"status":"ok","timestamp":1710487808313,"user_tz":-540,"elapsed":24,"user":{"displayName":"서나미","userId":"11215307119811085502"}},"outputId":"706968b8-2b75-49fc-df5b-b362db0779d9","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":43,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[-0.3587, -0.4704],\n","         [-0.3587, -0.4704],\n","         [-0.3587, -0.4704]],\n","\n","        [[-0.3587, -0.4704],\n","         [-0.3587, -0.4704],\n","         [-0.3587, -0.4704]]], grad_fn=<ViewBackward0>)"]},"metadata":{},"execution_count":43}]},{"cell_type":"code","source":["list(linear.parameters()) # Ax + b"],"metadata":{"id":"sEwriBfdf8Ri","executionInfo":{"status":"ok","timestamp":1710487808313,"user_tz":-540,"elapsed":21,"user":{"displayName":"서나미","userId":"11215307119811085502"}},"outputId":"3b7931a1-10c3-4811-d9ad-001d1844436a","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":44,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Parameter containing:\n"," tensor([[ 0.1739, -0.2881, -0.4729,  0.2340],\n","         [-0.1213, -0.1068,  0.1627, -0.4071]], requires_grad=True),\n"," Parameter containing:\n"," tensor([-0.0056,  0.0021], requires_grad=True)]"]},"metadata":{},"execution_count":44}]},{"cell_type":"markdown","source":["### Activation Function Layer\n","We can also use the `nn` module to apply activations functions to our tensors. Activation functions are used to add non-linearity to our network. Some examples of activations functions are `nn.ReLU()`, `nn.Sigmoid()` and `nn.LeakyReLU()`. Activation functions operate on each element seperately, so the shape of the tensors we get as an output are the same as the ones we pass in."],"metadata":{"id":"dZ7bFMWhf-iD"}},{"cell_type":"code","source":["sigmoid = nn.Sigmoid()\n","output = sigmoid(linear_output)\n","output"],"metadata":{"id":"Tih5zCa_gCci","executionInfo":{"status":"ok","timestamp":1710487808313,"user_tz":-540,"elapsed":17,"user":{"displayName":"서나미","userId":"11215307119811085502"}},"outputId":"85a187c0-4526-4fe8-e91b-e01f9b9138fe","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":45,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[0.4113, 0.3845],\n","         [0.4113, 0.3845],\n","         [0.4113, 0.3845]],\n","\n","        [[0.4113, 0.3845],\n","         [0.4113, 0.3845],\n","         [0.4113, 0.3845]]], grad_fn=<SigmoidBackward0>)"]},"metadata":{},"execution_count":45}]},{"cell_type":"markdown","source":["### Putting the Layers Together\n","So far we have seen that we can create layers and pass the output of one as the input of the next. Instead of creating intermediate tensors and passing them around, we can use `nn.Sequentual`, which does exactly that."],"metadata":{"id":"yKtWdnf6gElP"}},{"cell_type":"code","source":["block = nn.Sequential(\n","    nn.Linear(5, 3),\n","    nn.ReLU(),\n","    nn.Linear(3, 5),\n","    nn.Sigmoid()\n",")"],"metadata":{"id":"zw7r1hCKgI0C","executionInfo":{"status":"ok","timestamp":1710487808313,"user_tz":-540,"elapsed":14,"user":{"displayName":"서나미","userId":"11215307119811085502"}}},"execution_count":46,"outputs":[]},{"cell_type":"markdown","source":["### Optimization\n","We have showed how gradients are calculated with the `backward()` function. Having the gradients isn't enought for our models to learn. We also need to know how to update the parameters of our models. This is where the optomozers comes in. `torch.optim` module contains several optimizers that we can use. Some popular examples are `optim.SGD` and `optim.Adam`. When initializing optimizers, we pass our model parameters, which can be accessed with `model.parameters()`, telling the optimizers which values it will be optimizing. Optimizers also has a learning rate (`lr`) parameter, which determines how big of an update will be made in every step. Different optimizers have different hyperparameters as well."],"metadata":{"id":"bNzHpvrlgMk6"}},{"cell_type":"code","source":["# Create the y data\n","y = torch.ones(10, 5)\n","\n","# Add some noise to our goal y to generate our x\n","# We want out model to predict our original data, albeit the noise\n","x = y + torch.randn_like(y)\n","x"],"metadata":{"id":"55-S6EU6gPFH","executionInfo":{"status":"ok","timestamp":1710487808313,"user_tz":-540,"elapsed":13,"user":{"displayName":"서나미","userId":"11215307119811085502"}},"outputId":"333ad928-4d98-4dab-fff8-b0700755b626","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":47,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 2.2196,  1.7745,  3.6335, -0.1157,  2.3483],\n","        [-0.1487, -0.3550,  0.3920,  2.6643,  1.2875],\n","        [ 1.3407,  3.3309,  0.3421,  0.2369,  0.7445],\n","        [ 1.3046,  1.1808, -0.7258,  0.1861,  0.5696],\n","        [ 1.3840,  1.7666,  1.2018,  0.7487,  0.7294],\n","        [ 0.0048,  2.3275,  0.3536,  3.2583,  0.5383],\n","        [-0.1754,  0.7848, -0.7914,  0.4923,  0.4555],\n","        [ 0.9711,  1.2120,  0.0583, -0.7287,  0.4391],\n","        [ 0.7514,  1.6861,  0.8437,  3.2241,  0.7131],\n","        [ 0.1686,  0.6933,  2.4573,  0.9088,  0.8204]])"]},"metadata":{},"execution_count":47}]},{"cell_type":"code","source":["import torch.optim as optim\n","\n","# Define the optimizer\n","adam = optim.Adam(block.parameters(), lr=1e-1)\n","\n","# Define loss using a predefined loss function\n","loss_function = nn.BCELoss()\n","\n","# Calculate how our model is doing now\n","y_pred = block(x)\n","loss_function(y_pred, y).item()"],"metadata":{"id":"fes6lsbtgQzi","executionInfo":{"status":"ok","timestamp":1710487811219,"user_tz":-540,"elapsed":2917,"user":{"displayName":"서나미","userId":"11215307119811085502"}},"outputId":"4b4bc30a-f14b-4819-a5a8-5efadec18fa6","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":48,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.7555209398269653"]},"metadata":{},"execution_count":48}]},{"cell_type":"markdown","source":["Let's see if we can have our model achieve a smaller loss. Now that we have everything we need, we can setup our training loop."],"metadata":{"id":"sYcKt4IngSUm"}},{"cell_type":"code","source":["# Set the number of epoch, which determines the number of training iterations\n","n_epoch = 10\n","\n","for epoch in range(n_epoch):\n","  # Set the gradients to 0\n","  adam.zero_grad()\n","\n","  # Get the model predictions\n","  y_pred = block(x)\n","\n","  # Get the loss\n","  loss = loss_function(y_pred, y)\n","\n","  # Print stats\n","  print(f\"Epoch {epoch}: traing loss: {loss}\")\n","\n","  # Compute the gradients\n","  loss.backward()\n","\n","  # Take a step to optimize the weights\n","  adam.step()\n"],"metadata":{"id":"RGCtyOYJgUpG","executionInfo":{"status":"ok","timestamp":1710487811220,"user_tz":-540,"elapsed":18,"user":{"displayName":"서나미","userId":"11215307119811085502"}},"outputId":"0b6c5591-5bb4-411e-e54d-c58b6f682901","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":49,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0: traing loss: 0.7555209398269653\n","Epoch 1: traing loss: 0.6445321440696716\n","Epoch 2: traing loss: 0.5937466621398926\n","Epoch 3: traing loss: 0.5451932549476624\n","Epoch 4: traing loss: 0.4899294376373291\n","Epoch 5: traing loss: 0.4206928312778473\n","Epoch 6: traing loss: 0.34689366817474365\n","Epoch 7: traing loss: 0.26941123604774475\n","Epoch 8: traing loss: 0.1810774952173233\n","Epoch 9: traing loss: 0.10739384591579437\n"]}]},{"cell_type":"markdown","source":["You can see that our loss is decreasing. Let's check the predictions of our model now and see if they are close to our original `y`, which was all `1s`."],"metadata":{"id":"xH545PYNgW2s"}},{"cell_type":"code","source":["# See how our model performs on the training data\n","y_pred = block(x)\n","y_pred"],"metadata":{"id":"awruZQ1ygYBm","executionInfo":{"status":"ok","timestamp":1710487811220,"user_tz":-540,"elapsed":13,"user":{"displayName":"서나미","userId":"11215307119811085502"}},"outputId":"642e2c81-314e-432e-cf85-694adcfd3969","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":50,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0.9864, 0.9996, 0.9999, 0.9711, 0.9938],\n","        [0.9008, 0.9839, 0.9935, 0.8828, 0.9795],\n","        [0.9240, 0.9895, 0.9957, 0.9248, 0.9548],\n","        [0.8736, 0.9713, 0.9852, 0.8952, 0.9288],\n","        [0.9220, 0.9891, 0.9956, 0.9199, 0.9610],\n","        [0.8861, 0.9783, 0.9903, 0.8809, 0.9692],\n","        [0.7648, 0.8975, 0.9287, 0.8349, 0.8869],\n","        [0.8691, 0.9692, 0.9838, 0.8927, 0.9265],\n","        [0.9198, 0.9891, 0.9958, 0.9049, 0.9775],\n","        [0.9378, 0.9935, 0.9978, 0.9131, 0.9864]], grad_fn=<SigmoidBackward0>)"]},"metadata":{},"execution_count":50}]},{"cell_type":"code","source":["# Create test data and check how our model performs on it\n","x2 = y + torch.randn_like(y)\n","y_pred = block(x2)\n","y_pred"],"metadata":{"id":"734D6z96gZXx","executionInfo":{"status":"ok","timestamp":1710487811220,"user_tz":-540,"elapsed":10,"user":{"displayName":"서나미","userId":"11215307119811085502"}},"outputId":"1fa4134a-6ace-4199-fc7a-600aa1855ebf","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":51,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0.8185, 0.9423, 0.9665, 0.8496, 0.9367],\n","        [0.9311, 0.9917, 0.9969, 0.9197, 0.9743],\n","        [0.9351, 0.9926, 0.9973, 0.9225, 0.9758],\n","        [0.9279, 0.9908, 0.9965, 0.9189, 0.9711],\n","        [0.8786, 0.9736, 0.9866, 0.8979, 0.9314],\n","        [0.9633, 0.9976, 0.9993, 0.9414, 0.9892],\n","        [0.9077, 0.9851, 0.9936, 0.9073, 0.9605],\n","        [0.8516, 0.9615, 0.9795, 0.8726, 0.9410],\n","        [0.9543, 0.9962, 0.9988, 0.9357, 0.9845],\n","        [0.8266, 0.9466, 0.9689, 0.8615, 0.9259]], grad_fn=<SigmoidBackward0>)"]},"metadata":{},"execution_count":51}]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}